1.  Nginx
    - what is Nginx
      Nginx can be used as a web server that serves content or it can be used as reverse proxy which helps
      when load balancing. it can also help with Backend re-routing & caching

    - talk about what a normal architecture look like vs what an Nginx architecture would look like
      - normal architecture:
        [users request] -> [server] -> [database]
    
      - Nginx architecture
        [users request] -> [nginx]  -> [server] ->
                                    -> [server] -> [database]
                                    -> [server] ->
      
      with the normal architecture, the request are going straight to the severs and it is not the best,
      because if the request becomes too much for the server it will crash it, it is also difficult
      to scale with the normal architecture, because every time you add a new server in the backend,
      you need to start changing frontend code -> as this can become pretty complex. but with the
      Nginx architecture, everything becomes easy to manage, if you need to scale your backend servers,
      you'll only need to update your nginx config with the new backend ports

    - what are the following concepts/terms in Nginx? {Frontend, Backend, Upstream, Downstream}
        Frontend: refers to all communication between the user_request & Nginx

        Backend: refers to all communication between Nginx & the servers

        Upstream: refers to the servers or services that NGINX communicates with to fulfill a request
            When NGINX acts as a reverse proxy or load balancer, it forwards client requests to upstream
            servers (like application servers or backend services).
            The upstream servers handle the actual processing of the request and then return the result to
            NGINX,  which then sends the response back to the client.

        Downstream: refers to the end-users or the clients that send requests to your NGINX server.
            When a client sends an HTTP request to NGINX, NGINX processes it, and then sends the response
            back to the downstream client.

    - Tell us about the 7 layers in OSI Model
        Please - Physical layer 1
        Do - Data-link layer 2
        Not - Network layer 3
        Throw - transport (TCP/IP) layer 4
        Sausage - session layer 5
        Pizza - presentation layer 6
        Away - application layer 7

2. Talk about Layer 4 and Layer 7 proxying in Nginx
    Layer 4 (Transport Layer of the OSI model (TCP/UDP)):
      - Layer 4 proxying operates at the Transport Layer of the OSI model (TCP/UDP)
      - NGINX forwards raw TCP/UDP packets without inspecting the content
      - Used for protocols like MySQL, FTP, or custom services
      - Provides basic load balancing and traffic distribution based on IP and port

        e.g:
        stream {
            upstream backend {
                server backend1.example.com:3306;
                server backend2.example.com:3306;
            }

            server {
                listen 3306;
                proxy_pass backend;
            }
        }

    Layer 7 (Application Layer, HTTP proxying):
     - Layer 7 proxying works at the Application Layer and understands HTTP/HTTPS content
     - NGINX can inspect headers, cookies, and URLs to make routing decisions
     - Allows for more advanced features like URL rewriting, caching, and SSL termination

        e.g:
        http {
            upstream backend {
                server backend1.example.com;
                server backend2.example.com;
            }

            server {
                listen 80;

                location / {
                    proxy_pass http://backend;
                }
            }
        }
    
    You see that layer 7 in written inside the "http" block and layer 4 proxying is written
    in the "stream" block

3. Talk about TLS termination and TLS passthrough in Nginx
    1. TLS Termination:
        - TLS Termination refers to NGINX decrypting the incoming SSL/TLS traffic (i.e: HTTPS) and
          then forwarding the unencrypted traffic (HTTP) to the backend servers
        - NGINX handles the decryption process, offloading this task from the backend servers,
          making it more efficient
        - TLS Termination operates at Layer 7 because NGINX decrypts and inspects the traffic (HTTP)

        e.g:
        server {
            listen 443 ssl;
            server_name example.com;

            ssl_certificate /etc/nginx/ssl/cert.pem;
            ssl_certificate_key /etc/nginx/ssl/key.pem;

            location / {
                proxy_pass http://backend;  # Unencrypted HTTP forwarded to backend
            }
        }

    2. TLS Passthrough:
        - TLS Passthrough means that NGINX does not decrypt the incoming SSL/TLS traffic.
          Instead, it forwards the encrypted traffic directly to the backend server for decryption.
        - This is useful when you want to maintain end-to-end encryption between the client & the backend server
          (e.g., for security reasons or when the backend server needs to handle the SSL certificates).
        - NGINX acts as a transparent proxy in this case, forwarding encrypted traffic without inspecting
          or modifying it.
        - TLS Passthrough operates at Layer 4 because the encrypted traffic is passed through without
          decryption or inspection.

        e.g:
        stream {
            upstream backend {
                server backend1.example.com:443;  # Backend server with TLS (HTTPS)
                server backend2.example.com:443;  # Another backend for load balancing
            }

            server {
                listen 443;
                proxy_pass backend;  # Load balancing between backend1 and backend2
            }
        }

4.  Nginx Docker container
    - start a nginx container in detached mode using image: nginx:1.26.3, container_name: nginx, port 80:80
        docker run -d
            -p 80:80
            --name nginx
            --hostname nginx
            -v /fullPath/nginx.conf:/etc/nginx/nginx.conf
            nginx:1.26.3

    - inspect the container
        docker inspect 4e :: where "4e" is the first 2 letters of the container ID

    - see the logs from the container
        docker logs 4e
        docker logs -f 4e :: to follow the logs

    - attach to the container
        docker attach 4e

    - login into the container, go to etc, see users(in passwd), see groups (in group)
        docker exec -it -u root 4e bash
        cd etc
        cat passwd (adduser stanley)
        cat group (addgroup chukwu_family, usermod -aG chukwu_family stanley)

5.  Solving image one
    - Look at the image of the 2 nginx load balancers we will be creating
        see ./section1/4-nginx-docker/1img
        see ./section1/4-nginx-docker/2img

    - Create a node.js application to listen on port 9000, should console.log(the hostname)
        import express from "express"
        import os from "os"

        const app = express()
        const hostname = os.hostname
        const port = 9000

        app.get('/', (req, res) => {
            res.send(`Hello world ${hostname}`)
        })

        app.listen(port, () => {
            console.log(`Example app with: ${hostname}, listening on port ${port}!`);
        })

    - create the Dockerfile for the node.js application
        FROM node:alpine as dev_stage
        RUN addgroup app && adduser -S -G app app
        USER app
        WORKDIR /usr/src/app 
        COPY package*.json pnpm-lock.yml .
        RUN npm install
        COPY --chown=app:app . .
        RUN chown -R app:app /usr/src/app
        CMD ["npm", "run", "dev"]

    - create the docker image of the nodejs application
        docker build -t nodeapp_for_nginx:v1 -f Dockerfile.dev .

    - use the image to spin up 3 new containers and do not expose the ports, use names
      nodeapp1, nodeapp2, nodeapp3
        docker run -d --name nodeapp1 --hostname nodeapp1 nodeapp_for_nginx:v1
        docker run -d --name nodeapp2 --hostname nodeapp2 nodeapp_for_nginx:v1
        docker run -d --name nodeapp3 --hostname nodeapp3 nodeapp_for_nginx:v1

6. Now write you Nginx configuration:
    1. create upstream server for the nodeapp services
    2. create a server that listens on port 8080 and forward all request to the upstream server
    3. start the nginx container

    1&2:
    in nginx.conf
    http {
        upstream node_backend {
            server nodeapp1:9000;
            server nodeapp2:9000;
            server nodeapp3:9000;
        }

        server {
            listen 8080;

            server_name localhost; 

            location / {
                proxy_pass http://node_backend/;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;

                # Set timeouts to prevent hanging requests
                proxy_connect_timeout 90;
                proxy_send_timeout 90;
                proxy_read_timeout 90;
            }
        }
    }

    # Set worker connections
    events {
        worker_connections 1024;  # Increase connections for higher load handling
    }

    # Auto worker processes based on the number of CPU cores
    worker_processes auto;

    3:
        docker run -d
            --name nginx_test_server
            -p 80:8080
            -v ./nginx.conf:/etc/nginx/nginx.conf
            nginx:1.26.3

7. Read or explain the code in your nginx config
    http {}: inside the http block is where you configure everything about your nginx server
        when working with layer 7 proxying

    upstream {}: the upstream is used to group servers that nginx will be talking to at the "backend",
        you usually add a group name for each upstream you create, i.e upstream <group-name>.
        Nginx will distribute the request among the servers listed inside the upstream in a
        round-robin fashion by default.

    server {}: the server block is used to configure how Nginx will handle incoming request on
        the "frontend"
        - the listen 8080; tells Nginx to listen to request on port 8080
        - the location / {} & proxy_pass http://node_backend/; means all request to the root url /
            should be forwarded to the upstream "node_backend" group

    events {}: the event block is used to configure a-lot of settings, but here we use it to configure
        the maximum number of simultaneous connections that a worker process can handle.
        If your server receives more incoming connections than this limit allows, it might start to
        queue connections or reject them, depending on the load and configuration
        (such as keepalive_timeout or accept_mutex)

    worker_processes auto; this means nginx will automatically spawn worker processes based on the number
        of available CPU cores. each worker will be able to handle up to 1024 simultaneous connections
        on each worker_processes and depending on how many worker_connections you set for each worker_processes
        to handle

8. Now Networking and spinning up all the services
    - now create a Docker Network called backend_network_for_nginx
        docker network create backend_network_for_nginx

    - connects all the containers to the Network i.e (nginx_test_server, nodeapp1, nodeapp2, nodeapp3)
        docker stop nginx_test_server nodeapp1 nodeapp2 nodeapp3

        docker network connect backend_network_for_nginx nginx_test_server
        docker network connect backend_network_for_nginx nodeapp1
        docker network connect backend_network_for_nginx nodeapp2
        docker network connect backend_network_for_nginx nodeapp3

    - start all the nodeapp & the nginx container. see if everything is working as expected
        if the containers already exist, then start the container
            docker start nodeapp1 nodeapp2 nodeapp3 nginx_test_server
        
        if they do not exits, you have to create them before adding them to your docker network

9.  Solving image two problem
    - create another nginx container called ng2 using the same config, port 81:8080
        docker run
            -d
            --name ng2
            -p 81:8080
            -v ./nginx.conf:/etc/nginx/nginx.conf
            nginx:1.26.3

    - stop the container and connect it to the backend_network_for_nginx network
        docker stop ng2 nginx_test_server nodeapp1 nodeapp2 nodeapp3
        docker network connect backend_network_for_nginx ng2

    - start all the containers and test the different nginx on different ports
        docker start ng2 nginx_test_server nodeapp1 nodeapp2 nodeapp3

9.1 Read about exposing ports of your docker container
    when working with docker and nginx, the only ports that should be exposed should be only the nginx port,
    all other containers port should not be exposed

10. Now go and practice in real life from (question 5 to question 9)
    find the project template on my system @:
        D:\Sz - projects\28-devops\3-nginx\1-hussien-nasser-introduction-to-nginx\nginx-project-1

    find the project template on github @:
        https://github.com/stanleychukwu17/hussien-nasser-introduction-to-nginx-project-1

11. see answers in:
    ./section1/11-nginx-conf/nginx.conf

12. Understanding nginx timeouts for efficient configuration
    - Read: see list of frontend & backend timeouts
        - Frontend timeouts:
            client_header_timeout
            client_body_timeout
            send_timeout
            keepalive_timeout
            lingering_timeout
            resolver_timeout
        - Backend timeouts:
            proxy_connect_timeout
            proxy_send_timeout
            proxy_read_timeout
            keepalive_timeout
            proxy_next_upstream_timeout

    - Read: see explanation of frontend timeouts
      - client_header_timeout
        defines a timeout for reading client request header, if a client does not transmit the entire header
        within this time, the request is terminated with 408(Request timeout) error.
        default: 60s

      - client_body_timeout
        defines a timeout for reading client request body. the timeout is set only for a period between two
        successive read operations, not for the transmission of the whole request body.
        If a client does not send anything within this period, the request is terminated with a
        408(Request timeout) error
        default: 60s

      - send_timeout
        sets a timeout for transmitting a response to the client. the timeout is set only between two
        successive write operations, not for the transmission of the whole response. if the client does
        not receive anything within this time, the connection is closed
        default: 60s

      - keepalive_timeout
        sets a timeout for which the connection with nginx server should stay connected to the client
        without any communications, if this timeout is reach, the client is disconnected
        default: 75s

      - lingering_timeout
        specifies the maximum waiting time for more client data to arrive, if data are not received during
        this time, the connection is closed

      - resolver_timeout
       sets a timeout for dns name resolution
       default: 30s

    - Read: see explanation of backend timeouts
      - proxy_connect_timeout
        defines a timeout for establishing a connection with a proxied server.
        default 75s

      - proxy_send_timeout
        sets a timeout for transmitting of request to the proxied server. the timeout is set only between
        two successive write operations, not for the transmission of the whole request. if the proxied server
        does not receive anything within this time, the connection is closed

      - proxy_read_timeout
        defines a timeout for reading a response from the proxied server. the timeout is set only between two
        successive read operations, not for the transmission of the whole response. if the proxied server does
        not transmit anything within this time, the connection is closed

      - keepalive_timeout
        sets a timeout during which an idle keepalive connection to an upstream server will stay open

      - proxy_next_upstream_timeout
        Limits the time during which a request can be passed to the next server.
        the 0 value turns off this limitation, the default is 0
        read more {
            say you have 3 servers connected to nginx and nginx sends a request to server1 and server1 fails,
            nginx will try server2, if 2 fails, it will try server3. if 3 fails, it will go back to server1
            and try again, so this timeout can help to kil this loop otherwise nginx will continue to loop
            all through the 3 servers until 60s is reached
        }

13. Nginx as a layer 7 proxy, look at the example, explain what is going on and re-write it
      http {
        upstream all_apps {
            server app1_backend:2222;
            server app1_backend:3333;
            server app2_backend:3333;
            server app2_backend:4444;
        }

        upstream app1 {
            server app1_backend:2222;
            server app1_backend:3333;
        }

        upstream app2 {
            server app2_backend:3333;
            server app2_backend:4444;
        }

        server {
            listen 80;

            server_name localhost;

            location / {
                proxy_pass http://all_apps/;
            }

            location /app1 {
                proxy_pass http://app1/;
            }

            location /app2 {
                proxy_pass http://app2/;
            }

            location /admin {
                return 403;
                # or you can do: return 403 "You are forbidden from visiting this page";
            }
        }

        server {
            listen 79453;

            server_name localhost;

            location /admin {
                proxy_pass http://all_apps/;
            }
        }
      }

    - explanation
        in the example above, we have 3 upstream servers, when a user visits the root url (i.e localhost:80),
        nginx takes them to the upstream server "all_apps", if they visit localhost/app1, they are taken to
        "app1_backend", if they visit localhost/app2, they are taken to "app2_backend".
        but no one is allowed to visit /admin on port 80, i.e (localhost/admin), but if they go to port 79453
        (localhost:79453/admin), they'll be proxied to the right upstream

14. Nginx as a layer 4 proxy, look at the example, explain it and re-write it
    - code
      stream {
        upstream all_apps {
            server app1_backend:2222;
            server app1_backend:3333;
            server app2_backend:4444;
            server app2_backend:5555;
        }

        server {
            listen 80;
            proxy_pass http://all_apps/;
        }
      }

    - explanation
      with layer 4 proxying, you can't really do much, you're just passing the request to one of the servers

15. Enable HTTPS on Nginx
    - Read: how do you enable https on your nginx server
      if you want to enable https on nginx, you need a working dns name, now you follow the steps below

      1. obtain an SSL certificate
        - let us use certbot (will only work in a linux environment, will not work in your git bash environment)

        - sudo apt-get update
          sudo apt-get install -y certbot python3-certbot-nginx
          sudo certbot --nginx
        
        - certbot will automatically configure nginx to use SSL

      2. configure listening to https in your server block
        server {
            listen 443 ssl;
            server_name stanleychukwu.com

            ssl_certificate /etc/ssl/certs/your_certificate.crt;  # Path to your SSL certificate
            ssl_certificate_key /etc/ssl/private/your_private.key;  # Path to your private key

            ssl_protocols TLSv1.2 TLSv1.3;  # Enforce strong protocols
            ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384';  # Secure ciphers

            # Other configuration options as needed...
            location / {
                root /var/www/your-site;
                index index.html index.htm;
            }
        }

        # test the configuration for syntax error and reload nginx
        sudo nginx -t
        sudo systemctl reload nginx
    
      3. Optionally: redirect http to https
        if you want to automatically redirect http traffic to https, add the following block before the https
        server block:

        server {
            listen 80;
            server_name stanleychukwu.com

            return 301 https://$host$request_uri; # redirects to https
        }

      4. Additional Tip
        You can also configure strict transport security to enforce HTTPS for browsers. the below can be added to
        your HTTPS server block:

        server {
            listen 443 ssl;

            ...

            location / {
                ...

                add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always;
            }
        }
      
    - enable fast and secure TLS 1.3 on Nginx
        server {
            ...

            ssl_protocols TLSv1.3;
        }

    - enable http2 on Nginx
        server {
            listen 443 ssl http2;

            ssl_protocols TLSv1.3;
        }


## Websockets with Nginx

16. practice
    - create a normal websocket node app
    - bring the app to live and test it
    - create 4 containers of the app
    - use layer 4 proxy for the websocket app
    - read some of the explanation on what is going on


