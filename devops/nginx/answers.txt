1.  Nginx
    - what is Nginx
      Nginx can be used as a web server that serves content or it can be used as reverse proxy which helps
      when load balancing. it can also help with Backend re-routing & caching

    - talk about what a normal architecture look like vs what an Nginx architecture would look like
      - normal architecture:
        [users request] -> [server] -> [database]
    
      - Nginx architecture
        [users request] -> [nginx]  -> [server] ->
                                    -> [server] -> [database]
                                    -> [server] ->
      
      with the normal architecture, the request are going straight to the severs and it is not the best,
      because if the request become too much for the server it will crash it, it is also difficult
      to scale with the normal architecture, because every time you add a new server in the backend,
      you need to start changing frontend code -> as this can become pretty complex. but with the
      Nginx architecture, everything becomes easy to manage

    - what are the following concepts/terms in Nginx? {Frontend, Backend, Upstream, Downstream}
        Frontend: refers to all communication between the user_request & Nginx

        Backend: refers to all communication between Nginx & the servers

        Upstream: refers to the servers or services that NGINX communicates with to fulfill a request
            When NGINX acts as a reverse proxy or load balancer, it forwards client requests to upstream
            servers (like application servers or backend services).
            The upstream servers handle the actual processing of the request and then return the result to
            NGINX,  which then sends the response back to the client.

        Downstream: refers to the end-users or the clients that send requests to your NGINX server.
            When a client sends an HTTP request to NGINX, NGINX processes it, and then sends the response
            back to the downstream client.

    - Tell us about the 7 layers in OSI Model
        Please - Physical layer 1
        Do - Data-link layer 2
        Not - Network layer 3
        Throw - transport (TCP/IP) layer 4
        Sausage - session layer 5
        Pizza - presentation layer 6
        Away - application layer 7

2. Talk about Layer 4 and Layer 7 proxying in Nginx
    Layer 4 (Transport Layer of the OSI model (TCP/UDP)):
      - Layer 4 proxying operates at the Transport Layer of the OSI model (TCP/UDP)
      - NGINX forwards raw TCP/UDP packets without inspecting the content
      - Used for protocols like MySQL, FTP, or custom services
      - Provides basic load balancing and traffic distribution based on IP and port

        e.g:
        stream {
            upstream backend {
                server backend1.example.com:3306;
                server backend2.example.com:3306;
            }

            server {
                listen 3306;
                proxy_pass backend;
            }
        }

    Layer 7 (Application Layer, HTTP proxying):
     - Layer 7 proxying works at the Application Layer and understands HTTP/HTTPS content
     - NGINX can inspect headers, cookies, and URLs to make routing decisions
     - Allows for more advanced features like URL rewriting, caching, and SSL termination

        e.g:
        http {
            upstream backend {
                server backend1.example.com;
                server backend2.example.com;
            }

            server {
                listen 80;

                location / {
                    proxy_pass http://backend;
                }
            }
        }
    
    You see that layer 7 in written inside the "http" block and layer 4 proxying is written
    in the "stream" block

3. Talk about TLS termination and TLS passthrough in Nginx
    1. TLS Termination:
        - TLS Termination refers to NGINX decrypting the incoming SSL/TLS traffic (i.e., HTTPS) and
          then forwarding the unencrypted traffic (HTTP) to the backend servers
        - NGINX handles the encryption and decryption process, offloading this task from the backend servers,
          making it more efficient
        - TLS Termination operates at Layer 7 because NGINX decrypts and inspects the traffic (HTTP)

        e.g:
        server {
            listen 443 ssl;
            server_name example.com;

            ssl_certificate /etc/nginx/ssl/cert.pem;
            ssl_certificate_key /etc/nginx/ssl/key.pem;

            location / {
                proxy_pass http://backend;  # Unencrypted HTTP forwarded to backend
            }
        }

    2. TLS Passthrough:
        - TLS Passthrough means that NGINX does not decrypt the incoming SSL/TLS traffic.
          Instead, it forwards the encrypted traffic directly to the backend server for decryption.
        - This is useful when you want to maintain end-to-end encryption between the client & the backend server
          (e.g., for security reasons or when the backend server needs to handle the SSL certificates).
        - NGINX acts as a transparent proxy in this case, forwarding encrypted traffic without inspecting
          or modifying it.
        - TLS Passthrough operates at Layer 4 because the encrypted traffic is passed through without
          decryption or inspection.

        e.g:
        stream {
            upstream backend {
                server backend1.example.com:443;  # Backend server with TLS (HTTPS)
                server backend2.example.com:443;  # Another backend for load balancing
            }

            server {
                listen 443;
                proxy_pass backend;  # Load balancing between backend1 and backend2
            }
        }

4.  Nginx Docker container {please write down the code and also practice it}
    - start a nginx container in detached mode using image: nginx:1.26.3, container_name: nginx, port 80:80
        docker run -p 80:80 -d --name nginx nginx:1.26.3

    - inspect the container
        docker inspect 4e :: where "4e" is the first 2 letters of the container

    - see the logs from the container
        docker logs 4e
        docker logs -f 4e :: to follow the logs

    - attach to the container
        docker attach 4e

    - login into the container, go to etc, see users(in passwd), see groups (in group)
        docker exec -it -u root 4e bash
        cd etc
        cat passwd (adduser stanley)
        cat group (addgroup chukwu_family, usermod -aG chukwu_family stanley)

5.  Solving image one
    - Look at the image of the 2 nginx load balancers we will be creating
        see ./section1/4-nginx-docker/1img
        see ./section1/4-nginx-docker/2img

    - Create a node.js application to listen on port 9000, should console.log(the hostname)
        import express from "express"
        import os from "os"

        const app = express()
        const hostname = os.hostname
        const port = 9000

        app.get('/', (req, res) => {
            res.send(`Hello world ${hostname}`)
        })

        app.listen(port, () => {
            console.log(`Example app with: ${hostname}, listening on port ${port}!`);
        })

    - create the Dockerfile for the node.js application
        FROM node:alpine as dev_stage
        RUN addgroup app && adduser -S -G app app
        USER app
        WORKDIR /usr/src/app 
        COPY package*.json pnpm-lock.yml .
        RUN npm install
        COPY --chown=app:app . .
        RUN chown -R app:app /usr/src/app
        CMD ["npm", "run", "dev"]

    - create the docker image of the nodejs application
        docker build -t nodeapp_for_nginx .

    - use the image to spin up 3 new containers and do not expose the ports, use names
      nodeapp1, nodeapp2, nodeapp3
        docker run --name nodeapp1 -d nodeapp_for_nginx
        docker run --name nodeapp2 -d nodeapp_for_nginx
        docker run --name nodeapp3 -d nodeapp_for_nginx

6. Now write you Nginx configuration:
    1. create upstream server for the nodeapp services
    2. create a server that listens on port 8080 and forward all request to the upstream server
    3. start the nginx container

    1&2:
    in nginx.conf
    http {
        upstream node_backend {
            server nodeapp1:9000;
            server nodeapp2:9000;
            server nodeapp3:9000;
        }

        server {
            listen 8080;

            server_name localhost; 

            location / {
                proxy_pass http://node_backend/;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;

                # Set timeouts to prevent hanging requests
                proxy_connect_timeout 90;
                proxy_send_timeout 90;
                proxy_read_timeout 90;
            }
        }
    }

    # Set worker connections
    events {
        worker_connections 1024;  # Increase connections for higher load handling
    }

    # Auto worker processes based on the number of CPU cores
    worker_processes auto;

    3:
        docker run -d
            --name nginx_test_server
            -p 80:8080
            -v ./nginx.conf:/etc/nginx/nginx.conf
            nginx:1.26.3

7. Read or explain the code in your nginx config
    http {}: inside the http block is where you configure everything about your nginx server

    upstream {}: the upstream is used to group servers that nginx will be talking to at the "backend",
        you usually add a group name for each upstream you create, i.e upstream <group-name>.
        Nginx will distribute the request among the servers listed inside the upstream in a
        round-robin fashion by default.

    server {}: the server block is used to configure how Nginx will handle incoming request on
        the "frontend"
        - the listen 8080; tells Nginx to listen to request on port 8080
        - the location / {} & proxy_pass http://node_backend/; means all request should be forwarded
            to the "upstream node_backend group"

    events {}: the event block is used to configure a-lot of settings, but here we use it to configure
        the maximum number of simultaneous connections that a worker process can handle.
        If your server receives more incoming connections than this limit allows, it might start to
        queue connections or reject them, depending on the load and configuration
        (such as keepalive_timeout or accept_mutex)

    worker_processes auto; this means NGINX will automatically spawn worker processes based on the number
        of available CPU cores. each worker will be able to handle up to 1024 simultaneous connections,
        depending on the server’s load and how many worker processes are running

8. Now Networking and spinning up all the services
    - now create a Docker Network called backend_network_for_nginx
        docker network create backend_network_for_nginx

    - connects all the containers to the Network i.e (nginx_test_server, nodeapp1, nodeapp2, nodeapp3)
        docker stop nginx_test_server nodeapp1 nodeapp2 nodeapp3

        docker network connect backend_network_for_nginx nginx_test_server
        docker network connect backend_network_for_nginx nodeapp1
        docker network connect backend_network_for_nginx nodeapp2
        docker network connect backend_network_for_nginx nodeapp3

    - start all the nodeapp & the nginx container. see if everything is working as expected
        if the containers already exist, then start the container
            docker start nodeapp1 nodeapp2 nodeapp3 nginx_test_server
        
        if they do not exits, you have to create them before adding them to your docker network

9.  Solving image two problem
    - create another nginx container called ng2 using the same config, port 81:8080
        docker run
            -d
            --name ng2
            -p 81:8080
            -v ./nginx.conf:/etc/nginx/nginx.conf
            nginx:1.26.3

    - stop the container and connect it to the backend_network_for_nginx network
        docker stop ng2 nginx_test_server nodeapp1 nodeapp2 nodeapp3
        docker network connect backend_network_for_nginx ng2

    - start all the containers and test the different nginx on different ports
        docker start ng2 nginx_test_server nodeapp1 nodeapp2 nodeapp3

10. 