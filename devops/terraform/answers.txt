1. The terraform core workflow is
    Write:
    Plan:
    Apply

2.  Terraform providers are plugins that allow Terraform to interact with various APIs and services. Each provider is 
    responsible for understanding the API of a specific service (like AWS, Azure, Google Cloud, or even custom APIs)
    and managing the lifecycle of the resources defined in your Terraform configuration.
    more_explanation {
        Providers enable you to:

        Define Resources: Specify what infrastructure components you want to create, such as virtual machines, databases,
            and networking resources.
        Manage State: Keep track of the state of your resources, allowing Terraform to understand the current configuration
            and make necessary updates.
        Provision Infrastructure: Automate the creation, modification, and deletion of resources as defined in your
            configuration files.
    }

3.
    terraform {
        required_providers {
            aws = {
                source = "hashicorp/aws"
                version = "~>5.0"
            }
        }
    }

    provider "aws" {
        region = "eu-north-1"
        access_key = var.access_key
        secret_key = var.secret_key
    }

4.
    resource "aws_vpc" "main" {
        cidr_block = "10.0.0.0/16"
        enable_dns_support = true
        enable_dns_hostname = true
    }

    resource = is a block.. i.e a "resource block"
    cidr_block, enable_dns_hostname, enable_dns_support = arguments
    aws_vpc, main = label names
    aws_vpc = resource type.. i.e "aws_vpc resource type"
    main = local_name {used for local identification}

5. terraform init - this will download all of the required_providers

6. VPC -> Subnet -> Route table & Internet Gateway -> Security groups -> EC2 instance {
    For EC2 instance, we need:
        data for "ami" automatic fetching
        keyGen for ssh logging into your EC2 vm
}

7. resource "<provider>_<resource_type>" "<local_name>" {
    # arguments
}

8. Classless Inter Domain Routing

9.
    resource "aws_vpc" "main" {
        cidr_block = var.vpc_cidr_block
        enable_dns_support = true
        enable_dns_hostnames = true

        tags = {
            Name = "Dev Env ${var.main_vpc_name}"
        }
    }

10. - terraform plan
    - terraform apply
    - terraform plan -out=tfplan or terraform plan -out tfplan
    - terraform apply "tfplan" or terraform apply "tfplan" -auto-approve

11. - terraform apply -auto-approve
    - terraform fmt
    - terraform fmt <name_of_folder>
      terraform fmt -diff
      terraform fmt -recursive
    - terraform validate

12. - terraform destroy
    - terraform destroy -auto-approve
    - terraform destroy -target "aws_vpc.main"
    - terraform apply -replace="aws_vpc.main"

13. D:\Sz - projects\0-exercise\terraform\answers\13_a.tf

14. you use the variable block
    variable "vpc_cidr_block" {
        default = "10.0.0.0/16"
        description = "The cidr block for the main vpc for development stage"
        type = string
    }
    - you save variables in two file: variables.tf & terraform.tfvars

15. terraform apply -auto-approve -var="vpc_cidr_block=10.0.0.0/16" -var="subnet_cidr_block=10.0.10.0/24"

16. terraform.tfvars does not compile with the other terraform files(i.e .tf files) into a single module
    - terraform.tfvars stores only the values. no description, no type
    - variables.tf stores everything, the (default, description & type)

17. touch production.tfvars
    terraform apply -auto-approve -var-file=production.tfvars

18. here is how to use it in your main.tf:
    resource "aws_vpc" "main" {
        cidr_block = var.vpc_cidr_block

        tags = {
            Name = "${var.environment_stage} VPC"
        }
    }

    - resource "aws_subnet" "web_subnet" {
        cidr_block = var.subnet_cidr_block
        vpc_id = aws_vpc.main.id
        availability_zone = var.subnet_availability_zone_1

        tags = {
            Name = "${var.environment_stage} subnet for VPC"
        }
    }

19. this is how they are applied into your config files
    -var & -var-file
    terraform.tfvars & production.tfvars
    environment variables "TF_VAR_*"

20. A route table is a virtual router within your vpc, it controls routing for all subnets within the vpc
    An Internet Gateway: it allows communication between instances in a Virtual Private Cloud (VPC) and
        the internet

21. - An aws_security_group is an aws managed service that determines if internet traffic are permitted
        into or out of an instance
    - aws_security_group = You have to create this one by yourself using the resource "aws_security_group"
      aws_default_security_group = This one comes by default when you create the VPC, it is always better to leave
        this one alone and create new security groups (i.e aws_security_group)

22. in variables.tf
    variable "general_public_address" {
        description = "everyone is allowed to visit this EC2 instance"
        type = string
    }
    in terraform.tfvars
    general_public_address = "0.0.0.0/0"
    ----
    resource "aws_security_group" "ec2_security" {
        vpc_id = aws_vpc.main.id
    
        # for ssh connections
        ingress {
            from_port = 22
            to_port = 22
            protocol = "tcp"
            cidr_block = [var.general_public_address]
        }

        # for http connection
        ingress {
            from_port = 80
            to_port = 80
            protocol = "tcp"
            cidr_block = [var.general_public_address]
        }

        # for outgoing request
        egress {
            from_port = 0
            to_port = 0
            protocol = "-1" # any type of request
            cidr_block = ["0.0.0.0/0"]
        }
    
        tags = {
            Name = "EC2 security group"
        }
    }

23.
    resource "aws_instance" "my_vm" {
        ami =                           "ami-4094940048585fse2"
        instance_type =                 "t2.micro"
        subnet_id =                     aws_subnet.web_subnet.id
        vpc_security_group_ids =        [aws_security_group.ec2_security.id]
        associate_public_ip_address =   true
        key_name =                      "production_ssh_key"

        tags = {
            Name = "My EC2 instance - Selling town EC2_1"
        }
    }

24. ssh-keygen -t rsa -b 2048 -C 'test key' -N '' -f ~/.ssh/test_rsa {
        -t rsa: Specifies the type of key to create, in this case, RSA.
        -b 2048: Sets the number of bits in the key, here 2048 bits.
        -C 'test key': Adds a comment to the key.
        -N '': used for encryption. Sets an empty passphrase for the key (not recommended for all use cases due to security reasons).
        -f ~/.ssh/test_rsa: Specifies the filename of the key file. and where to save the key
    }

25. chmod 400 ~/.ssh/test_rsa

26. 
    resource "aws_key_pair" "test_ssh_key" {
        key_name = "testing_ssh_key"
        public_key = file(<path_to_file>) (i.e C:/Users/STANLEY/.ssh/test_rsa.pub)
    }

    resource "aws_instance" "my_vm" {
        ...
        key_name = aws_key_pair.test_ssh_key.key_name
        ...
    }

27. ssh -i ~/.ssh/test_rsa ec2-user@<PublicIP>

28. 
    in variables.tf we do:
    variable "ssh_pubic_key_path" {
        description = "The path to where the ssh key is stored on my local computer"
        type = string
    }

    in terraform.tfvars we do:
    ssh_pubic_key_path = "C:/Users/STANLEY/.ssh/test_rsa.pub"

    in main.tf we do:
    resource "aws_key_pair" "test_ssh_key" {
        key_name = "testing_ssh_key"
        public_key = file(var.ssh_pubic_key_path)
    }

    Note: if you change the public_key value, a new instance is created, each instance belong to the key
        it was created with

29. Data sources are like a function, and their goal is to fetch data from cloud providers. An example is a
    list of ami images from an availability_zone
    -
    data "aws_ami" "latest_amazon_linux2" {
        owners = ["amazon"]
        most_recent = true   # if more than one image is returned, terraform will use the most recent image

        filter {
            name = "name"    # filter the result using the items in the value[] list below
            values = ["amzn2-ami-kernel-*-x86_64-gp2"]
        }

        filter {
            name = "architecture"
            values = ["x86_64"]
        }
    }
    {Now we need to update our aws_instance with the dynamic ami id}
    resource "aws_instance" "my_vm" {
        ami = data.aws_ami.latest_amazon_linux2.id
        ...
    }

30. outputs.tf (you can also do output.tf)

31. in your outputs.tf
    output "ec2_public_ip_address" {
        description = "The public IP address of the EC2 instance."
        value       = aws_instance.my_vm.public_ip
    }

    output "ec2_ami" {
        description = "The ami of our ec2 instance"
        value = aws_instance.my_vm.ami
    }

    output "vpc_id" {
        description = "ID of the main VPC"
        value = aws_vpc.main.id
    }

    output "ec2_instance_id" {
        description = "The ID of the EC2 instance."
        value       = aws_instance.my_vm.id
    }

32. {
    - terraform output
    - terraform output ec2_public_ip_address
    - terraform output -json
    - terraform output -json > output.json
    - if you want to hide an output from display in the terminal, set sensitive = true, i.e {
        output "ec2_public_ip_address" {
            description = "The public IP address of the EC2 instance for ebay"
            value = aws_instance.my_vm.public_ip
            sensitive = true
        }
    }
}

33. Terraform state is a critical component of Terraform's infrastructure management process. It is a file that
    tracks the current state of your infrastructure as managed by Terraform. Terraform state acts as d source of
    truth for your managed infrastructure, enabling Terraform to track, update, and manage resources effectively.
    For more {
        1. **Purpose**: The state file maintains a mapping between your Terraform configuration and the actual
            resources deployed in your infrastructure. It helps Terraform understand what resources exist, their
            configurations, and how they relate to each other.

        2. **Storage**: By default, the state file is named `terraform.tfstate` and is stored locally in your
            working directory. However, in team environments or production settings, it’s common to use remote
            state storage solutions like Amazon S3, Azure Blob Storage, or Terraform Cloud for better management
            and collaboration.

        3. **Usage**: During operations like `terraform apply`, Terraform reads the state file to determine the
            changes that need to be made to your infrastructure. It then updates the state file with the latest
            information after applying the changes.

        4. **Locking and Security**: For remote state, locking mechanisms prevent simultaneous updates.
    }
    ---
    ---
    terraform state is stored in a terraform.tfstate file

34. - terraform show
    - terraform show | grep -A 20 aws_vpc -: means, show all resources, then pipe the result to the grep
        command and then search for aws_vpc, if you find the result, then show 20lines after it(i.e A20)
    - terraform show -target=<resource_address>
        e.g terraform show -target=aws_instance.my_vm
    - terraform show <plan_file> e.g terraform show tfplan

35. - terraform state list
    - terraform state show <resource_address>
        e.g terraform state show aws_instance.my_vm
    - terraform state pull
    - terraform state list | grep 'module.<module_name>'

36. running commands on you ec2 instance
    - which is the best for running commands on your ec2 instance (cloud-init or ansible)
        - For initial boot time configuration, i.e if you need to perform simple one time setup tasks when a VM is
            first launched, then use "cloud-init". it is the best tool. it is lightweight, easy to use and designed
            for exactly this Purpose
        - For ongoing configuration management, if you need to manage the configuration of servers throughout their
            lifecycle, including making changes, applying updates, or orchestrating complex tasks, Then use Ansible,
            it provides more power, flexibility and scalability
        # using both
            Many organization use both tools in tandem:
            ~ cloud-init for initial bootstrapping and configuration e.g installing docker, apache, httpd & ansible.
            ~ ansible for ongoing configuration management and complex orchestration after the initial setup is completed

    - Read: read about some of the highlights of using cloud-init
        ~ it is the standard for customizing cloud instances on spin up
        ~ it runs on most linux distributions and cloud providers
        ~ cloud-init can run per-instance or per-boot configurations

    - create a cloud-init configuration in yaml called "web-app-template.yaml" and use it in your ec2
        ~ for the creation, see the file in 
            - ./task/web-app-template-v1.yml - from teacher
            - ./task/web-app-template-v2.yml - from codeium
            - ./task/web-app-template-4rm-chatGPT.yml - from chatGPT

            - So now, we will use a "data resource block" to load the the configuration, so in main.tf, we do:
                data "template_file" "user_data" {
                    # path to where the configuration is located relative to your main.tf
                    template = file("./task/web-app-template-v1.yml")
                }
            - we have to add this to our "aws_instance" configuration
            resource "aws_instance" "my_vm" {
                ...
                # user_data = file("entry-script.sh") he commented this line out
                user_data = data.template_file.user_data.rendered
                ...
            }

            With all of this, whenever a new ec2_instance setup is completed, terraform will run the code in
            our web-app-template and set_up the ec2_instance in accordance to the set_up in the file

    - explain the code in your cloud-init configuration
        -
        1. Adding groups to the system
        groups:
            - devops: [root, sys]
            - hashicorp
        The above is adding 2 groups to the system, it added the "devops" group with 2 users "root" & "sys",
        and also created another group "hashicorp" but without any users in this group

        -
        2. Adding users to the system. Users are added after groups are added
        users:
            - default               # this will be the default system user
            - name: terraform       # the name of the user
                gecos: terraform    # the GECOS field(General Electric Comprehensive Operating Supervisor)
                                        typically contains the full-name or the description of the user, but
                                        here, we used the same name as the username "terraform"
                shell: /bin/bash    # the default shell of the user
                primary_group: hashicorp        # adds a default group for the user
                sudo: ALL=(ALL) NOPASSWD:ALL    # this allows the user to execute any command as any user
                                                    (including as root user) without any password
                groups: users, admin    # this adds the user to other groups (users & admin group)
                lock_passwd: false      # this means the user account is not locked, so the user will not be
                                            required a password before they can login
                ssh_authorized_keys:
                    - ssh-rsa <key>
                    # the ssh_authorized_keys section allows you to Specify ssh-keys for the user, enabling
                        secure ssh access without using a password
                    # for the ssh-rsa <key>, replace the <key> with your rsa public key
        
        -
        3. Downloading and installing packages
        packages:
            - httpd
            - docker
        The above will download and install 2 packages, Apache httpd and Docker

        -
        4. Running commands on the system
        runcmd:
            - sudo systemctl start httpd
            - sudo systemctl start docker
            - sudo usermod -a -G docker ec2-user
            - sudo docker run -p 8080:80 nginx
        sudo: using sudo grants you all privileges so you can run & any commands you want
        systemctl: is a command used to control the systemd and system service manager
        "sudo usermod -a -G docker ec2-user":
            usermod: is used to modify the privileges and attributes of a user
            -a : user to append a user to a group without removing the user from another group
            -G : compliments -a, Specifies the group that the user should be added to
            {the "ec2-user" was added to the "docker" group, which allows the ec2-user to run any docker commands
             without needing to use sudo}
        "sudo docker run -p 8080:80 nginx":
            spins up a new docker container using an "nginx" image, the container will listen to request on port 8080,
            and map the request to nginx port 80

37.