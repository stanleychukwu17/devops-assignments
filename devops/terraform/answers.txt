1. The terraform core workflow is
    Write
    Plan
    Apply

2.  Terraform providers are plugins that allow Terraform to interact with various APIs and services.

    more_explanation {
        Providers enable you to:

        Define Resources: Specify what infrastructure components you want to create, such as virtual machines,
            databases, and networking resources.
        Manage State: Keep track of the state of your resources, allowing Terraform to understand the current 
            configuration and make necessary updates.
        Provision Infrastructure: Automate the creation, modification, and deletion of resources as
            defined in your configuration files.
    }

3.
    terraform {
        required_providers {
            aws = {
                source = "hashicorp/aws"
                version = "~>5.0"
            }
        }

        backend "s3" {
            bucket = var.bucket_name
            key = "backend.tfstate"
            region = "eu-north-1"
            encrypt = true
            dynamodb_table = var.table_name
        }
        or
        backend "remote" {
            organization = "my-terraform"
            workspaces {
                name = "Github-CI/CD"
            }
        }
    }

    provider "aws" {
        region = "eu-north-1"
        access_key = var.access_key
        secret_key = var.secret_key
    }

3.1 Read: more info on working with backend "s3" or backend "remote"
    * backend "s3" must be defined statically — Terraform doesn't support using variables
      (e.g: var.bucket_name, etc.) in the backend block directly or anywhere in the terraform block
      -
      You must define them directly or use a -backend-config file or CLI args
      -
      the "terraform" block is parsed before variables are loaded, which is why it doesn't support var.* directly

    e.g of using a "backend.config" file:
        inside your file, you add your backend config there:
        bucket         = "my-bucket"
        key            = "backend.tfstate"
        region         = "eu-north-1"
        encrypt        = true
        dynamodb_table = "my-lock-table"

        then re-initialize your terraform:
        terraform init -backend-config=backend.config

        and when you run "terraform apply", backend.config is not included in the final main module, only
        the .tf files will be in the final module, but be careful not to push the file to your github repo

4.
    resource "aws_vpc" "main" {
        cidr_block = "10.0.0.0/16"
        enable_dns_support = true
        enable_dns_hostnames = true
    }

    resource = is a block.. i.e a "resource block"
    cidr_block, enable_dns_hostnames, enable_dns_support = arguments
    aws_vpc, main = label names
    aws_vpc = resource type.. i.e "aws_vpc resource type"
    main = local_name {used for local identification}

5. terraform init - this will download all of the required_providers

6. VPC -> Subnet -> Internet Gateway & Route table -> Security groups -> EC2 instance {
    For EC2 instance, we need:
        data for "ami" automatic fetching
        keyGen for ssh logging into your EC2 vm
        user_data = for initial vm set_up and installation of needed services to run the vm
}

7. resource "<provider>_<resource_type>" "<local_name>" {
    # arguments
}

8. Classless Inter Domain Routing
8.1 see ./answers/8.1_cidr_block.txt

9.
    resource "aws_vpc" "main" {
        cidr_block = "10.0.0.0/16"
        enable_dns_support = true
        enable_dns_hostnames = true

        tags = {
            Name = "Dev Env ${var.main_vpc_name}"
        }
    }

10. - terraform plan
    - terraform apply
    - terraform plan -out=tfplan or terraform plan -out tfplan
    - terraform apply "tfplan" or terraform apply "tfplan" -auto-approve

11. - terraform apply -auto-approve
    - terraform fmt
    - terraform fmt <name_of_folder>
      terraform fmt -diff
      terraform fmt -recursive
    - terraform validate

12. - terraform destroy
    - terraform destroy -auto-approve
    - terraform destroy -target "aws_vpc.main"
    - terraform apply -replace="aws_vpc.main"

13. ./answers/13_a.tf

14.
    - How do you declare a variable, what are the three & four arguments in a variable declaration
        with 3 arguments:
        variable "vpc_cidr_block" {
            default = "10.0.0.0/16"
            description = "The cidr block for the main vpc for development stage"
            type = string
        }
        with 4 arguments
        variable "av_zones" {
            description = "A list of availability_zones"
            type = list(string)
            default = ["...", "...", "..."]
            sensitive = true
        }

    - you save variables in two file: variables.tf & (terraform.tfvars or <any_name>.auto.tfvars)

15. terraform apply -auto-approve -var="vpc_cidr_block=10.0.0.0/16" -var="subnet_cidr_block=10.0.10.0/24"

16. - what can you say about terraform.tfvars file?
      terraform.tfvars does not compile with the other terraform files(i.e .tf files) into a single module.
      so variable value in your .tfvars is safe

    - what is the difference btw terraform.tfvars and variables.tf
      - terraform.tfvars stores only the values. no description, no type
      - variables.tf stores everything, the (default(i.e the value), description & type)

    - Read: Read about *.auto.tfvars & terraform.tf
      Terraform automatically loads variables from files named exactly:
        - terraform.tfvars
        - terraform.tfvars.json
        - All *.auto.tfvars or *.auto.tfvars.json
        All of the above are loaded automatically without needing -var-file flag.
      ---
      But if you name your file something else like: "my-vars.tfvars" or "production.tfvars", they'll not be
      loaded automatically except you add .auto, i.e "my-vars.auto.tfvars" or "production.auto.tfvars"
    
17. touch production.tfvars
    terraform apply -auto-approve -var-file=production.tfvars
    or
    terraform apply -auto-approve -var-file="production.tfvars"

18. here is how to use it in your main.tf:
    resource "aws_vpc" "main" {
        cidr_block = var.vpc_cidr_block

        tags = {
            Name = "${var.environment_stage} VPC"
        }
    }

    - resource "aws_subnet" "web_subnet" {
        cidr_block = var.subnet_cidr_block
        vpc_id = aws_vpc.main.id
        availability_zone = var.subnet_availability_zone_1

        tags = {
            Name = "${var.environment_stage} subnet for VPC"
        }
    }

19. this is how they are applied into your config files
    -var & -var-file
    *.auto.tfvars
    terraform.tfvars & production.tfvars
    environment variables "TF_VAR_*"

20. A route table is a virtual router within your vpc, it controls routing for all subnets within the vpc
    An Internet Gateway: it allows communication between instances in a Virtual Private Cloud (VPC) and
        the internet

21. - An aws_security_group is an aws managed service that determines if internet traffic are permitted
        into or out of an instance, main used to control who and what gets access to your ec2 instances.
        it is also used for aws load balancer, mainly used to determine who gets access to the load balancer

    - aws_security_group: You have to create this one by yourself using the resource "aws_security_group"
      aws_default_security_group: This one comes by default when you create the VPC, it is always better to leave
        this one alone and create new security groups (i.e aws_security_group)

22. 
    - add a variable to your variables.tf and terraform.tfvars file: "public_cidr"
        in variables.tf
        variable "public_cidr" {
            description = "everyone is allowed to visit this EC2 instance"
            type = list(string)
            default = ["0.0.0.0/0"]
        }
        in terraform.tfvars
        public_cidr = ["0.0.0.0/0"]

    - use the variable to create an aws_security_group
        resource "aws_security_group" "ec2_security" {
            vpc_id = aws_vpc.main.id
        
            ingress = [
                # for ssh connections
                {
                    from_port = 22
                    to_port = 22
                    protocol = "tcp"
                    cidr_blocks = var.public_cidr
                    description = "for ssh connections to the ec2"
                    ipv6_cidr_blocks = []
                    self             = false
                    security_groups  = null
                    prefix_list_ids  = null
                },
                # for http connection on port 80
                {
                    from_port = 80
                    to_port = 80
                    protocol = "tcp"
                    cidr_blocks = var.public_cidr
                    description = "for http connections to the ec2 on port 80"
                    ipv6_cidr_blocks = []
                    self             = false
                    security_groups  = null
                    prefix_list_ids  = null
                }
                # for https connection on port 443
                {
                    from_port = 443
                    to_port = 443
                    protocol = "tcp"
                    cidr_blocks = var.public_cidr
                    description = "for http connections to the ec2 on port 443"
                    ipv6_cidr_blocks = []
                    self             = false
                    security_groups  = null
                    prefix_list_ids  = null
                }
            ]

            # for all outgoing request
            egress = [{
                from_port = 0
                to_port = 0
                protocol = "-1" # any type of request
                cidr_blocks = ["0.0.0.0/0"]
                description = "rule for all outgoing request from the ec2, all traffic are allowed"
                ipv6_cidr_blocks = []
                self             = false
                security_groups  = null
                prefix_list_ids  = null
            }]
        
            tags = {
                Name = "EC2 security group"
            }
        }

    - Read: explanation of some of the options used in the ingress and egress
        1. ipv6_cidr_blocks = [] ?
          - What is ipv6_cidr_blocks:
            IPv6 CIDR blocks specify which range of IPv6 addresses are allowed to access the resource.
          - Why is it empty ([]) ?
            The ipv6_cidr_blocks is empty because the rule is likely intended for IPv4 connections only. Since no
            IPv6 addresses are specified, the field is left empty (or set to an empty list []). If you wanted to
            allow IPv6 access, you would provide a valid IPv6 CIDR block here, such as "2001:0db8::/32".

        2. self = false
          - What is self ?
            The self field specifies whether the rule applies to traffic originating from the same security group.
            When self = true, the rule allows traffic between instances that are associated with the same security
            group. This is useful for cases where you want to allow internal communication between instances in
            the same security group.

        3. security_groups = null
          - What is security_groups?
            The security_groups field is used to specify which other security groups can send traffic to
            the instances associated with this rule.
          - Why is it set to null?
            Setting security_groups = null means that this rule is not specifically restricting traffic based
            on other security groups.

        4. prefix_list_ids = null
          - What is prefix_list_ids?
            The prefix_list_ids field allows you to specify AWS-managed prefix lists
            (collections of IP address ranges) to use in the rule. AWS offers managed prefix lists that
            include ranges for services like Amazon S3, CloudFront, and others. This allows you to define rules
            based on these predefined IP address ranges rather than manually specifying CIDR blocks
          - Why is it set to null?
            Setting prefix_list_ids = null means that the rule is not using any AWS-managed prefix lists.
            If this field were set with the ID of a prefix list (e.g., "pl-12345678"), it would allow traffic
            from the IP addresses included in that prefix list.

        5. Summary of All Fields:
          - ipv6_cidr_blocks = []: No IPv6 addresses are specified, implying the rule is for IPv4 traffic.
          - self = false: Traffic from instances within the same security group is not allowed.
          - security_groups = null: No specific security groups are specified, meaning the rule does not restrict
            based on other security groups
          - prefix_list_ids = null: No AWS-managed prefix lists are being used, meaning no predefined IP address
            ranges are referenced
          - {Each of these fields can be customized to control traffic flow based on different security needs}

23.
    resource "aws_instance" "my_vm" {
        ami =                           "ami-4094940048585fse2" or data.aws_ami.latest_amazon_linux2.id
        instance_type =                 "t3.micro"
        subnet_id =                     aws_subnet.web_subnet.id
        vpc_security_group_ids =        [aws_security_group.ec2_sg.id]
        associate_public_ip_address =   true
        key_name =                      aws_key_pair.test_ssh_key.key_name
        user_data =                     file("./entry_script.sh")
        count = 2

        tags = {
            Name = "My EC2 instance - Selling town EC2_1"
        }
    }

24. ssh-keygen -t rsa -b 2048 -C 'test key' -N '' -f ~/.ssh/test_rsa {
        -t rsa: Specifies the type of key to create, in this case, RSA.
        -b 2048: Sets the number of bits in the key, here 2048 bits.
        -C 'test key': Adds a comment to the key.
        -N '': used for encryption. Sets an empty passphrase for the key (not recommended for all use cases due to security reasons).
        -f ~/.ssh/test_rsa: Specifies the filename of the key file and where to save the key
    }

25.
    chmod 400 ~/.ssh/test_rsa
    this will set the file to something like:
    -r--r--r-- 1 .............. test_rsa
    -r--r--r-- 1 .............. test_rsa.pub

    if the file is not read_only for all the users, aws will not use it, it will cause an error

26. 
    resource "aws_key_pair" "test_ssh_key" {
        key_name = "testing_ssh_key"
        public_key = file(<path_to_file>) (i.e C:/Users/STANLEY/.ssh/test_rsa.pub)
    }

    resource "aws_instance" "my_vm" {
        ...
        key_name = aws_key_pair.test_ssh_key.key_name
        ...
    }

27.
    - ssh -i ~/.ssh/test_rsa ec2-user@<PublicIP>
    - sudo su - (i.e sudo switch user - where dash(-)=root)

28. 
    in variables.tf we do:
    variable "ssh_pubic_key_path" {
        description = "The path to where the ssh key is stored on my local computer"
        type = string
    }

    in terraform.tfvars we do:
    ssh_pubic_key_path = "C:/Users/STANLEY/.ssh/test_rsa.pub"

    in main.tf we do:
    resource "aws_key_pair" "test_key" {
        key_name = "testing_ssh_key"
        public_key = file(var.ssh_pubic_key_path)
    }

    Note: if you change the public_key value, a new instance is created, each instance belong to the key
        it was created with

29. Data sources are like a function, and their goal is to fetch data from cloud providers. An example is a
    list of ami images from an availability_zone
    -
    data "aws_ami" "amazon_img" {
        owners = ["amazon"]
        most_recent = true   # if more than one image is returned, terraform will use the most recent image

        filter {
            name = "name"    # filter the result using the items in the value[] list below
            values = ["amzn2-ami-kernel-*-x86_64-gp2"]
        }

        filter {
            name = "architecture"
            values = ["x86_64"]
        }
    }
    {Now we need to update our aws_instance with the dynamic ami id}
    resource "aws_instance" "my_vm" {
        ami = data.aws_ami.amazon_img.id
        ...
    }

30.
    outputs.tf (you can also do output.tf)

31. in your outputs.tf
    output "ec2_public_ip_address" {
        description = "The public IP address of the EC2 instance."
        value       = aws_instance.my_vm.public_ip
    }

    output "ec2_ami" {
        description = "The ami of our ec2 instance"
        value = aws_instance.my_vm.ami
    }

    output "vpc_id" {
        description = "ID of the main VPC"
        value = aws_vpc.main.id
    }

    output "ec2_instance_id" {
        description = "The ID of the EC2 instance."
        value       = aws_instance.my_vm.id
    }

32.
    - terraform output
    - terraform output ec2_public_ip_address
    - terraform output -json
    - 
      - terraform output -json > output.json
          Will not display output to the terminal, but will save output to output.json
      - terraform output -json | tee output.json
          Display the output in the terminal & Save the output into output.json
      - terraform output -json | tee output.json > /dev/null
          Will not display output to the terminal, but will save output to output.json

    - if you want to hide an output from display in the terminal, set sensitive = true, i.e {
        output "ec2_public_ip_address" {
            description = "The public IP address of the EC2 instance for ebay"
            value = aws_instance.my_vm.public_ip
            sensitive = true
        }
    }

33. Terraform state is a critical component of Terraform's infrastructure management process. It is a file
    that tracks the current state of your infrastructure as managed by Terraform. Terraform state acts
    as the source of truth for your managed infrastructure, enabling Terraform to track, update, and
    manage resources effectively.
    For more {
        1. **Purpose**: The state file maintains a mapping between your Terraform configuration and the
            actual resources deployed in your infrastructure. It helps Terraform understand what resources
            exist, their configurations, and how they relate to each other.

        2. **Storage**: By default, the state file is named `terraform.tfstate` and is stored locally in your
            working directory. However, in team environments or production settings, it’s common to use
            remote state storage solutions like Amazon S3, Azure Blob Storage, or Terraform Cloud for
            better management and collaboration.

        3. **Usage**: During operations like `terraform apply`, Terraform reads the state file to determine
            the changes that need to be made to your infrastructure. It then updates the state file with
            the latest information after applying the changes.

        4. **Locking and Security**: For remote state, locking mechanisms prevent simultaneous updates.
    }
    ---
    ---
    terraform state is stored in a terraform.tfstate file

34. - terraform show
    - terraform show | grep -A 20 aws_vpc -: means, show all resources, then pipe the result to the grep
        command and then search for aws_vpc, if you find the result, then show 20lines after it(i.e A20)
    - terraform show -target=<resource_address>
        e.g terraform show -target=aws_instance.my_vm
    - terraform show <plan_file> e.g terraform show tfplan

35. terraform state
    - terraform state list
    - terraform state show <resource_address>
        e.g terraform state show aws_instance.my_vm
    - terraform state pull
    - terraform state list | grep 'module.<module_name>'
      e.g: terraform state list | grep -i -A20 "module.vpc_module"

36. running commands on you ec2 instance
    - which is the best for running commands on your ec2 instance (cloud-init or ansible)
        - For initial boot time configuration, i.e if you need to perform simple one time setup tasks when a VM is
            first launched, then use "cloud-init". it is the best tool. it is lightweight, easy to use and designed
            for exactly this Purpose
        - For ongoing configuration management, if you need to manage the configuration of servers throughout their
            lifecycle, including making changes, applying updates, or orchestrating complex tasks, Then use Ansible,
            it provides more power, flexibility and scalability
        # using both
            Many organization use both tools in tandem:
            ~ cloud-init for initial bootstrapping and configuration e.g installing docker, apache, httpd & ansible.
            ~ ansible for ongoing configuration management and complex orchestration after the initial setup is completed

    - create an entry-script.sh configuration that can be used in your ec2 instance
        ./answers/36.2-entry-script.sh

    - simple question: why do you start docker and enable docker.. what is the start and enable command
        - when you do "systemctl start docker", the docker service is started on your linux machine,
          so now you can run your docker commands.. otherwise if the docker service is not started, you will not
          be able to run docker commands.
        
        - "systemctl enable docker", now say that your docker service is running and for some reasons your server
          crashes and restarts, after the restart, your docker will be disabled because it is waiting for you to
          run a command of "systemctl start docker", but if you do "systemctl enable docker" during boot-time
          configuration, every time your server restarts, docker service will also start immediately

37.
    - explain why this error occurred and how can it be solved
        so the main issue is that this is a new_project and all the terraform code here was copied from another
        terraform project_1, and in "project_1", the "aws_key_pair" has already been used.
        for every new aws_key_pair key_name you use, aws creates a new entry for it, so now this new_project
        is trying to create a new aws_key_pair using the same key_name that already exist on your aws account,
        that's why an error occurred
        -
    - how you solve the above problem
        To solve this problem {the teacher says it is usually a common problem}, either you use another key_name or
        you import the existing aws_key_pair into this new_project, here is what you'll do:

        terraform import aws_key_pair.<local_name> <key_pair_name>
        -
        <local_name>: is the name you want to give to the resource in your Terraform configuration
        <key_pair_name>: is the actual name of the key pair in AWS.

        e.g:
        terraform import aws_key_pair.my_key_pair testing_ssh_key
        -
        resource "aws_key_pair" "my_key_pair" {
            key_name   = "testing_ssh_key"
            public_key = file("<path_to_file>")
        }
    - Read: read how you can find your key pairs on aws
        - Go to your aws console, go to the vpc section
        - Go to your Network & Security tab
        - Click on Key Pairs {it's a Link - here you'll see all d key pairs you've used in all ur different projects}

38.
    - login to your ec2 instance
        ssh -i ~/.ssh/test_rsa ec2-user@<PublicIP>

    - verify if all the users were created successfully
        tail /etc/passwd
        -
        you'll see stuffs like:
        terraform:x:1000:1000:terraform:/home/terraform:/bin/bash           # the user we created with cloud-init
        ec2-user:x:1001:1003:EC2 Default user :/home/ec2-user:/bin/bash     # default user created by aws_instance
        apache:x:48:48 ...                                                  # user created by apache

    - verify if all the groups were created successfully
        tail /etc/group
        -
        hashicorp:x:1000                # 1
        devops:x:1001:root              # 2
        admin:x:1002:terraform          # 3
        ec2-user:x:1003                 # 4
        apache:x:48                     # 5
        docker:x:992:ec2-user           # 6
        -
        1. Group name: hashicorp, Password: x, Group ID (GID): 1000, Members: No members
        2. Group name: devops, Password: x, Group ID (GID): 1001, Members: "root user"
        3. Group name: admin, Password: x, Group ID (GID): 1002, Members: "terraform user"
        4. Group name: ec2-user, Password: x, Group ID (GID): 1003, Members: No members
        5. Group name: apache, Password: x, Group ID (GID): 48, Members: No members
        6. Group name: docker, Password: x, Group ID (GID): 992, Members: "ec2-user user"

    - check if your httpd is running
        systemctl status httpd

    - check if your docker is running
        systemctl status docker

    - use your browser and confirm that the ec2 instance is accessible from the web
        http://35.159.20.29/
    - use your browser and confirm that the docker on your ec2 instance is running
        http://35.159.20.29:8080/

39. Read: read about how to troubleshot & logging in terraform
    ~ Always run "terraform fmt" and "terraform validate" before you run terraform "plan" or "apply".
    ~ To log your terraform operations in your production environment, you have to set your "TF_LOG"
        to either TRACE, DEBUG, INFO, WARN or ERROR 
    ~ Then to save all your logs to a file, you have to set TF_LOG_PATH to filename:
        e.g
            in your terminal do:
            export TF_LOG=DEBUG
            export TF_LOG_PATH=terraform.log
    ~ there are also other options such as TF_LOG_CORE & TF_LOG_PROVIDER, you can research them at your
        own convenience

================
SECTION 2 BEGINS
================
1. List the different value types in terraform
    - List simple types
        number, string, bool, null
    - List collection types
        list, map, set
    - List structural types
        tuple, object

2. Collection types
    - Give an example of using "list"
        variable "av_zones" {
            description = "availability_zones in the region"
            type = list(string)
            default = ["eu-central-1a", "eu-central-1b", "eu-central-1c"]
        }
        ---
        in main.tf
        resource "aws_subnet" "web_subnet" {
            availability_zone = var.av_zones[1] # eu-central-1b
        }

    - Give an example of using "maps"
        variable "amis" {
            description = "..."
            type = map(string)
            default = {
                "eu-central-1a" = "ami-something-eu-central"
                "us-west-1" = "ami-something-us-west-1"
            }
        }
        ---
        in main.tf
        resource "aws_instance" "my_vm" {
            ami = var.amis[var.region]
            or
            ami = var.amis["${var.region}"]
        }

3. Structural types
    - Give an example of using "tuple"
        variable "my_vm" {
            description = "..."
            type = tuple([string, number, bool])
            default = ["t2.micro", 1, true]
        }
        ---
        in main.tf
        resource "aws_subnet" "my_vm" {
            ami = var.amis[var.region]
            instance_type = var.my_vm[0]
            count = var.my_vm[1]
            associate_public_ip_address = var.my_vm[2]
        }

    - Give an example of using "object"
        variable "egress_dsg" {
            type = object({
                from_port = number
                to_port = number
                protocol = string
                cidr_blocks = list(string)
            })
            default = {
                from_port = 0,
                to_port = 65365, # maximum amount of ports
                protocol = "tcp",
                cidr_blocks = ["100.0.0.0/16", "200.0.0.0/16"]
            }
        }
        ---
        in main.tf
        resource "aws_security_group" "ec2_security" {
            ...
            egress {
                from_port = var.egress_dsg["from_port"]
                to_port = var.egress_dsg["to_port"]
                protocol = var.egress_dsg["protocol"]
                cidr_blocks = var.egress_dsg["cidr_blocks"]
            }
        }

4. The count meta-argument
    - What is count in terraform and what can it be used for
        - count is used to manage similar resources, you can use it to automatically scale up/down resources
        - count and for_each are looping techniques

    - Use count to create 3 aws instances
        resource "aws_instance" "my_vm" {
            ami = ....
            instance_type = "t3.micro"
            count = 3
        }

    - Using count & variable: "users" (list(string), a list of usernames), create 4 IAM users
        variable "users" {
            type = list(string)
            default = ["demo-user", "stanley", "edward", "chukwu"]
        }

        resource "aws_iam_user" "level2user" {
            name = "${element(var.users, count.index)}"
            path = "/system/"
            count = "${length(var.users)}"
        }

        {You can go to the terraform doc to see how you can customize the IAM users and set level/roles and what
          the users can have access to}

    - explain the element and length functions
        ~ element is used to retrieve an item from a list
        ~ length returns the total number of items in a list

    - when you do count=0 in any resource, what will happen when you run terraform apply
        when you set count = 0 in a resource block, that resource will not be created, if the resource already
        exists, the resource will be destroyed

5. 
    - Read & practice: the for_each meta-argument {No need to master}
        for_each was introduced to overcome the downside of count, it works with either sets or maps
        e.g:
        variable "users" {
            type = list(string)
            default = ["demo-user", "stanley", "edward", "chukwu"]
        }
        -
        resource "aws_iam_user" "test" {
            for_each = var.users
            name = each.key # each.key iterates over the list and grabs the key
        }
        ---
        you can also do:
        resource "aws_iam_user" "test" {
            for_each = var.users
            name = each.value # when working with list or sets, each.value = each.key
        }

    - Read & practice: for_each with a map
        variable "users" {
            type = map(string)
            default = {
                alice  = "alice-dev"
                bob    = "bob-prod"
                charlie = "charlie-test"
            }
        }
        ---
        resource "aws_iam_user" "test" {
            for_each = var.users
            name     = each.value    # ← correct: uses the map value
            # name = each.key       # ← would use the map key instead
        }

6.  
    - Read: take a look at looping with dynamic block
        dynamic block is another way of looping that can save us time
        e.g:
        variable "ingress_ports" {
            description = "List of Ingress Ports"
            type = list(number)
            default = [22, 80, 25, 443, 993, 8080]
        }
        -
        in main.tf
        resource "aws_security_group" "ec2_security" {
            vpc_id = aws_vpc.main.id

            dynamic "ingress" {
                for_each = var.ingress_ports
                content {
                    from_port = ingress.value
                    to_port = ingress.value
                    protocol = "tcp"
                    cidr_blocks = ["0.0.0.0/0"]
                }
            }

            egress {
                from_port = 0
                to_port = 0
                protocol = "-1"
                cidr_blocks = ["0.0.0.0/0"]
            }
        }

    - Read: what happens when you run terraform apply
        if you run terraform apply, your security group will be created with all the ports provided, you can
        go to your aws console to confirm this:
        ~ go to aws > search for "VPC" > SECURITY > SECURITY_GROUPS
        ~ select the security group you created, click on inbound rules
            here you'll see all the rules you've created with the dynamic block

    - Read: changing the iterator name
        you can set an iterator name to use in accessing the for_each values
        dynamic "ingress" {
            for_each = var.ingress_ports
            iterator = iport
            content {
                from_port = iport.value
                to_port = iport.value
                protocol = "tcp"
                cidr_blocks = ["0.0.0.0/0"]
            }
        }

    - Read: see the explanation for the different ports used
        1. Port 22:
            - Service: SSH (Secure Shell)
            - Purpose: Used for secure remote login and command execution on a server.

        2. Port 80:
            - Service: HTTP (Hypertext Transfer Protocol)
            - Purpose: Used for unencrypted web traffic, typically for websites that don't require security.

        3. Port 25:
            - Service: SMTP (Simple Mail Transfer Protocol)
            - Purpose: For sending email btw mail servers. It’s often blocked by ISPs for outgoing mail to prevent spam.

        4. Port 443:
            - Service: HTTPS (Hypertext Transfer Protocol Secure)
            - Purpose: For secure web traffic, which encrypts the communication btw a web server & a client (browser).

        5. Port 993:
            - Service: IMAPS (Internet Message Access Protocol Secure)
            - Purpose: Used for secure access to email inboxes. It is an encrypted version of IMAP,
                       typically for retrieving email from a mail server.

        6. Port 8080:
            - Service: HTTP Alternate (often used for proxies or web applications)
            - Purpose: A common port for web servers, often used as an alternative to port 80.
                       It may be used for proxy servers or other applications that require HTTP access.

7. Conditional Expressions
 - let's say we want to spin up different instances depending on our current environment, use
    terraform conditions and variables to check if we should launch an ec2_instance for testing
    or for production
    -
    variable "is_test" {
        description = "..."
        type = bool
    }
    -
    in terraform.tfvars
    is_test = true
    -
    in main.tf
    resource "aws_instance" "test_server" {
        ami = "..."
        instance_type = "t3.micro"
        count = var.is_test == true ? 1 : 0
    }
    resource "aws_instance" "production_server" {
        ami = "..."
        instance_type = "t3.large"
        count = var.is_test == false ? 1 : 0
    }

    You can also do stuffs like:
    instance_type = var.env_stage == "dev" ? "t3.micro" : "t3.large"

8. Locals
    - in what files can you save your locals
        You can define your locals in a file called locals.tf, you can also store them in variables.tf and
        lastly in main.tf(i.e if you do not have too many locals)

    - give an example of using locals
        in locals.tf
        locals {
            owner = "DevOps Corp Team"
            project = "online store"
            cidr_blocks = ["172.16.10.0/24", "172.16.20.0/24", "172.16.30.0/24"]
            dns_support = true
            common-tags = {
                Name = "Dev"
                Environment = "development"
                version = 1.10
            }
        }
        -
        in main.tf
        resource "aws_vpc" "dev_vpc" {
            cidr_block = "10.0.0.0/16"
            enable_dns_support = local.dns_support
            enable_dns_hostnames = local.dns_support
            tags = local.common-tags
        }
        resource "aws_subnet" "dev_subnet" {
            vpc_id = aws_vpc.dev_vpc.id
            cidr_block = local.cidr_blocks[0]
            availability_zone = "eu-central-1a"
        }
        resource "aws_internet_gateway" "dev_igw" {
            vpc_id = aws_vpc.dev_vpc.id
            tags = {
                Name = local.common-tags["Name"]
                version = local.common-tags["Version"]
            }
        }

9. Terraform built-in functions
    If you go to the terraform documentation, navigate to the functions section, you'll see all of the
    functions provided by terraform. You cannot create your own custom functions in terraform

    - lookup function
        it retrieves a "value" from a "map" using a "key" from the map
        lookup(<map>, <key_name>)
        e.g ami = lookup(var.ami, var.region)

    - difference btw lookup & element function
        lookup retrieves an item from a map using the key of the value: lookup(map, key)
        element retrieves an item from a list using the index of the value: element(list, index)

    - file function
        The file function reads the content of a file and returns it as a string
        e.g user_data = file("entry-script.sh")

    - formatDate & timestamp()
        timestamp() is used to retrieve the current time
        formatDate() is used in to format the date received from timestamp()
        e.g:
        in locals.tf
          locals {
            time = formatDate("DD MM YYYY hh:mm", timestamp())
          }
        in outputs.tf
          output "CurrentTime" {
            description = "Current Date and Time"
            value = local.time
          }

10. Give an example of using splat expressions
    Lets say for example we have
    resource "aws_instance" "my_vm" {
        ami = "ami..."
        instance_type = "t3.micro"
        count = 3
        associate_public_ip_address = true
    }

    The above will create 3 ec2 instances, so how do we dynamically display the info of these aws_instance's.
    Say we wanted to display the ip addresses, we might do:
    output "ip_address_1" {
        value = aws_instance.my_vm[0].public_ip
    } 

    imagine if we created upTo 10vm, this would take forever, so we can use splat-expressions to dynamically
    loop through the vm's details, i.e:
    output "ip_addresses" {
        value = aws_instance.my_vm[*].public_ip
    }

11. Read: Terraform remote state
    - working with amazon s3 to store your state file
      you can use terraform to create your s3 bucket or you can go to the aws website and create your bucket there
      after creating the s3_bucket and dynamodb for state locking, add the below to your terraform "main.tf" or
      add it to another file called "backend.tf":

      terraform {
        required_providers { ... }

        backend "s3" {
          bucket = <name-of-bucket>
          key = <name-of-file> e.g s3_backend.tfstate # the file will be saved with this name in your bucket
          region = "eu-north-1"
          encrypt = true
          dynamodb_table = <name-of-table>
        }
      }

    - using dynamodb to implement state locking
      You can go to aws and create a dynamodb table from there, or you can use terraform to create a
      dynamodb_table, The dynamodb_table will help with state locking to prevent simultaneous updating
      of a state file

    - Read: working with terraform cloud to store your state file
        if you want to use terraform cloud for you .tfstate file storage, then here you go:
        -
        backend "remote" {
          organization = "glitch-stream"
          workspaces {
            name = "ci-cd-staging"
          }
        }

11.1
    - list all the resources
        D:\Sz - projects\28-devops\1-terraform\02-terraform-web-app-from-devops-directive\tree.txt
    - write the code for all the resources listed
        D:\Sz - projects\28-devops\1-terraform\02-terraform-web-app-from-devops-directive\main.tf

12. Modules
    - what is a module in terraform
        A module is a set of terraform configuration files in a single directory.
        Modules are a key to writing reuseable, maintainable, testable terraform code
    - what is a root module & child module
        - root module: A root module is the place where you run terraform init, plan or apply
        - child module: All modules imported from other directories into the root module
    - what is the difference btw a local module and a remote module
        - Local modules: It is a module created by you or your team members and are loaded locally
        - Remote modules: There are modules that are loaded from remote sources such as terraform registry
            and are created and maintained by either hashicorp and it's partners or by 3rd parties

13. How do you structure your project to use modules
        root
        |__ project/
        |    |__ main.tf
        |    |__ variables.tf
        |    |__ terraform.auto.tfvars
        |    |__ outputs.tf
        |    |__ locals.tf
        |
        |__ modules/
            |__ vpc/
            |    |__ main.tf
            |    |__ outputs.tf
            |    |__ variables.tf
            |
            |__ ec2/
            |    |__ main.tf
            |    |__ outputs.tf
            |    |__ variables.tf
            |
            |__ networking/
                |__ main.tf
                |__ outputs.tf
                |__ variables.tf

14.
    - 4.1, 4.2, 4.3, 4.4
      All answers in: ./answers/14-modules-ec2/

14.1 what do you do after including a new module to your root module
      Anytime you import a new child module into a root module, you have to re-initialize your project:
      : "terraform init"

15. Read: what makes a good module
    - Group resources in a logical fashion
    - Exposes input variable to allow necessary customization
    - Provides useful defaults
    - Return outputs to make further integrations possible

16. As you separate into modules, what resources goes into which modules?
    - vpc module
        aws_vpc, aws_subnet, aws_internet_gateway, aws_route_table, aws_route_table_association
    - ec2 module
        aws_security_group, aws_key_pair, data.aws_ami, aws_instance
    - storage module
        aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration
    - network module
        aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_lb_listener_rule,
        aws_security_group.alb_sg, aws_lb
    - dns module
        aws_route53_zone, aws_route53_record
    - database module
        aws_db_instance

17. Accessing child modules output
    - expose your vpc_id using an output from your vpc module and access it from the root module
        in ./modules/vpc/outputs.tf
            output "vpc_id" {
                value = aws_vpc.main.id
            }
        in ./projects/main.tf
            module "vpc_module" {
                source = "../module/vpc"
                ...
            }
        in ./projects/outputs.tf
            output "vpc_id" {
                description = "..."
                value = module.vpc_module.vpc_id
            }

    - what is the syntax for accessing the output of a child module
        module.<module_name>.<output_name>

    - how will different child modules access their outputs e.g:
      use the vpc_id from the "vpc_module" in the "ec2_module" to create the aws_security_group
        in ./modules/ec2/main.tf
            resource "aws_security_group" "ec2" {
                vpc_id = var.vpc_id
                ...
            }
        -
        in ./modules/ec2/variables.tf
            variable "vpc_id" {

            }
        -
        in ./projects/main.tf
            module "ec2_module" {
                source = "../modules/ec2"
                vpc_id = module.vpc_module.vpc_id
            }

    - how do you expose an entire resource as an output, e.g:
      - expose the entire ec2 instance in the ec2 module
        in ./modules/ec2/outputs.tf
            output "ec2_outputs" {
                value = aws_instance.my_vm
            }

      - access the values from the root module, get: ec2_ami, ec2_public_ip_address, ec2_instance_id
        in ./projects/outputs.tf
            output "ec2_ami" {
                value = module.ec2_module.ec2_outputs.ami
            }
            output "ec2_public_ip_address" {
                value = module.ec2_module.ec2_outputs.public_ip
            }
            output "ec2_instance_id" {
                value = module.ec2_module.ec2_outputs.id
            }

17.1 see what the final module of DevOps Directive looked like
    in projects/main.tf
    terraform { ... }
    provider "aws" { ... }
    -
    locals {
        env = terraform.workspace
    }
    -
    module "web_app" {
        source = "../../modules/web-app-module/"
        # input variables
        bucket_name = "devops-data-${local.env}"
        domain = "stanleychukwu.com"
        env = local.env
        instance_type = "t3.micro"
        create_dns_zone = terraform.workspace == "production" ? true : false
        db_name = "${local.env}-DB"
        db_user = "foo"
        db_pass = var.db_pass
    }
    -
    So here you see that he kinda abstracted all the modules into one single module

18. - Convert all the code/project from DevOps Directive to use modules

19. Terraform registry
    - Read: what is terraform registry
        Terraform registry is like npm, where you can find different modules and providers to use in your
        terraform project

    - Read: How to create a vpc using a module from terraform registry
        - he went to the terraform registry website @registry.terraform.io/modules/ and he searched for "vpc"
          and then selected "terraform-aws-modules/vpc"
        - under the readme section of the module, he copied the example there and pasted it on his main.tf,
          then he parameterized the values, the final result look something like:
            module "vpc" {
                source = "terraform-aws-modules/vpc/aws"
                name = local.project_name
                cidr_block = var.vpc_cidr_block
                ...
            }
          
          this module created a vpc, subnets, internet gateway & a route table

    - Read: How to create a security group using a module from terraform registry
        - search for "security group", then select "terraform-aws-modules/security-group"
        - he copied one of the examples and the final result looked like:
          module "sh_server_sg" {
            source = "terraform-aws-modules/security-group/aws/modules/ssh"
            name = "ssh"
            description = "security group for ssh ports open within the vpc"
            vpc_id = module.vpc.vpc_id

            ingress_cidr_blocks = ["0.0.0.0/0"]
          }

    - Read: How to create an ec2 instance using a module from terraform registry
        - Go and search for "ec2", select any of the module from the drop down, go through it's guide
          on setting up an ec2, and there you go!

    - Read: Accessing outputs from the modules
        - Go through each of the doc of the modules you're using to see how it exposes output, e.g for the
          vpc module we used earlier on, the the vpc_id would look like:
          -
          module.vpc.vpc_id

    - Read: ssh-ing into the aws_instance created using this module from terraform registry
        you cannot use the ec2-user as usual to ssh into the ec2 instance created from a module,
        the ec2-user is only available when you use the amazon-linux-images, if you're using a
        remote module to create an ec2 instance, then check the docs to know the right user to use for
        ssh-ing into the instance


================
SECTION 3 BEGINS
================
1. Drawing of images
    - Look at the image of the ec2 from lecture 1 and draw it on your own
        - see ./answers/1-draw-images/img1.jpg

    - Look at the image from DevOps Directive lecture and draw it on your own
        - see ./answers/1-draw-images/img2.jpg

2. How do you create an s3_bucket and dynamodb_table for storing and locking of .tfstate files
    - for s3_bucket
      resource "aws_s3_bucket" "bucket {
        bucket          = <unique_name_of_bucket>
        force_destroy   = true
      }

      resource "aws_s3_bucket_server_side_encryption_configuration" "encrypt" {
        bucket = aws_s3_bucket.bucket.bucket

        rule {
            apply_server_side_encryption_by_default {
                sse_algorithm = "AES256"
            }
        }
      }
    
    - for dynamodb
      resource "aws_dynamodb" "state_locking" {
        name            = "terraform-tfstate-locking"
        billing_mode    = "PAY_PER_REQUEST"
        hash_key        = "LOCKID"

        attribute {
            name = "LOCKID"
            type = "S"
        }
      }

3. Meta arguments in terraform
    - depends_on
        when you use depends_on, terraform automatically generates dependency graph based on reference.
        this will makes sure that resources that are depended on are created first before the ones that
        depends on it
        e.g:
        resource "aws_iam_role" "example" {
            name = "example"
            assume_role_policy = "..."
        }

        resource "aws_iam_instance_profile" "example" {
            role = aws_iam_role.example.name
        }

        resource "aws_iam_role_policy" "example" {
            name = "example"
            role = aws_iam_role.example.name
            policy = jsonencode({
                "statement" = [{
                    "Action" = "s3:*"
                    "Effort" = "Allow"
                }]
            })
        }

        resource "aws_instance" "example" {
            ami = "ami..."
            instance_type = "t3.micro"
            iam_instance_profile = aws_iam_instance_profile.example

            depends_on = [
                aws_iam_role_policy.example
            ]
        }

    - lifecycle:
        arguments you can use in lifecycle are: "create_before_destroy", "ignore_changes", "prevent_destroy"
        -
        create_before_destroy: can help with zero downtime, where a new resource is created before the current
                                one is destroyed
        ignore_changes: prevents terraform from trying to revert meta-data being set elsewhere
        prevent_destroy: causes terraform to reject any plan which would destroy the resource
        -
        e.g:
        resource "aws_instance" "my_vm" {
            ...
            instance_type = "t3.micro"

            lifecycle {
                create_before_destroy = true
                ignore_changes = [
                    tags
                ]
                prevent_destroy = true
            }
        }

4. Managing multiple environment

    One-config: --------------------------------------------------------------
                |                           |                           |
            development                   staging                   production

    - workspaces:
        this involves you using multiple named sections within a single backend
        e.g staging, development, production e.t.c

    - File structure: talk about it
        this involves using of directory layout, where you can use the same module in different sub-directories.
        variables in the different directories will determine what resources are created

        e.g:
        |__ modules
            |
            |__ vpc_module
            |
            |__ ec2_module
        |
        |__ development
            |__ main.tf, variables.tf, terraform.tfvars
        |
        |__ production
            |__ main.tf, variables.tf, terraform.tfvars
        |
        |__ staging
            |__ main.tf, variables.tf, terraform.tfvars

        some of the advantages of using file structure is, you can reference resources from different
        configurations(i.e entirely separate configurations), this is made possible by calling the different
        "config remote state file"
        e.g: say you deployed some ec2 instances in project-A and you need the i.p addresses of the
            ec2_instances in project-B, you can reference the "remote state file" of project-A from
            project-B to get the i.p address

5. Read: How to use terraform workspace to manage environment 
    - creating a workspace
        terraform workspace new <name-of-workspace>
        e.g: terraform workspace new development
             terraform workspace new production

    - see all workspace
        terraform workspace list

    - switching btw workspaces
        terraform workspace switch <name-of-workspace>
        e.g: terraform workspace switch production
             terraform workspace switch development

    - deleting of workspaces
        terraform workspace delete <name-of-workspace>
        e.g: terraform workspace delete development
             terraform workspace delete production

6.
    Read: Terragrunt
        This is a tool provided by grunt-work.io, it provide utilities to make certain terraform use
        cases easier
        - keeping terraform code DRY
        - executing commands across multiple TF configs
        - working with multiple cloud accounts
        -
        it's a very great tool, once you start scaling up, you can research more on how to use this tool

    Read: using the "tree" command  
        this one is just a terminal tip: you can use the "tree" command to see all your directories and
        sub-directories
        tree -a; # to see both hidden directories
        tree -L2; # to limit the sub-directories level show to 2
        -
        this command will only work in a pure linux environment, it works in my ubuntu-20.24(WSL), it
        won't work in my "git bash", but it works in macBook since they ship with linux

    Read: using Directory/File structure to manage environment
        you can have something like:
        -
        infra/
            modules/
                vpc_module/
                ec2_module/
            global/
                main.tf, variables.tf, terraform.auto.tfvars, outputs.tf
            production/
                main.tf, variables.tf, terraform.auto.tfvars, outputs.tf
            staging/
                main.tf, variables.tf, terraform.auto.tfvars, outputs.tf

        the "global/main.tf" housed only resources that were shared by both production & staging.
        in  global/main.tf, you have:
        -
        terraform { ... }
        provider "aws" { ... }
        resource "aws_route53_zone" "primary" {
            name = "stanleychukwu.com"
        }

7. Testing terraform code
    see ./answers/7-terraform-test-code/
    - using bash: look at the code and write by yourself
    - using golang: look at the code and write by yourself


