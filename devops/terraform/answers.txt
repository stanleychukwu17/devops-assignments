1. The terraform core workflow is
    Write:
    Plan:
    Apply

2.  Terraform providers are plugins that allow Terraform to interact with various APIs and services. Each provider is 
    responsible for understanding the API of a specific service (like AWS, Azure, Google Cloud, or even custom APIs)
    and managing the lifecycle of the resources defined in your Terraform configuration.
    more_explanation {
        Providers enable you to:

        Define Resources: Specify what infrastructure components you want to create, such as virtual machines, databases,
            and networking resources.
        Manage State: Keep track of the state of your resources, allowing Terraform to understand the current configuration
            and make necessary updates.
        Provision Infrastructure: Automate the creation, modification, and deletion of resources as defined in your
            configuration files.
    }

3.
    terraform {
        required_providers {
            aws = {
                source = "hashicorp/aws"
                version = "~>5.0"
            }
        }
    }

    provider "aws" {
        region = "eu-north-1"
        access_key = var.access_key
        secret_key = var.secret_key
    }

4.
    resource "aws_vpc" "main" {
        cidr_block = "10.0.0.0/16"
        enable_dns_support = true
        enable_dns_hostnames = true
    }

    resource = is a block.. i.e a "resource block"
    cidr_block, enable_dns_hostnames, enable_dns_support = arguments
    aws_vpc, main = label names
    aws_vpc = resource type.. i.e "aws_vpc resource type"
    main = local_name {used for local identification}

5. terraform init - this will download all of the required_providers

6. VPC -> Subnet -> Route table & Internet Gateway -> Security groups -> EC2 instance {
    For EC2 instance, we need:
        data for "ami" automatic fetching
        keyGen for ssh logging into your EC2 vm
        user_data = for initial vm set_up and installation of needed services to run the vm
}

7. resource "<provider>_<resource_type>" "<local_name>" {
    # arguments
}

8. Classless Inter Domain Routing
8.1 see ./answers/8.1_cidr_block.txt

9.
    resource "aws_vpc" "main" {
        cidr_block = var.vpc_cidr_block
        enable_dns_support = true
        enable_dns_hostnames = true

        tags = {
            Name = "Dev Env ${var.main_vpc_name}"
        }
    }

10. - terraform plan
    - terraform apply
    - terraform plan -out=tfplan or terraform plan -out tfplan
    - terraform apply "tfplan" or terraform apply "tfplan" -auto-approve

11. - terraform apply -auto-approve
    - terraform fmt
    - terraform fmt <name_of_folder>
      terraform fmt -diff
      terraform fmt -recursive
    - terraform validate

12. - terraform destroy
    - terraform destroy -auto-approve
    - terraform destroy -target "aws_vpc.main"
    - terraform apply -replace="aws_vpc.main"

13. D:\Sz - projects\0-exercise\terraform\answers\13_a.tf

14. you use the variable block
    variable "vpc_cidr_block" {
        default = "10.0.0.0/16"
        description = "The cidr block for the main vpc for development stage"
        type = string
    }
    - you save variables in two file: variables.tf & terraform.tfvars

15. terraform apply -auto-approve -var="vpc_cidr_block=10.0.0.0/16" -var="subnet_cidr_block=10.0.10.0/24"

16. terraform.tfvars does not compile with the other terraform files(i.e .tf files) into a single module
    - terraform.tfvars stores only the values. no description, no type
    - variables.tf stores everything, the (default, description & type)

17. touch production.tfvars
    terraform apply -auto-approve -var-file=production.tfvars

18. here is how to use it in your main.tf:
    resource "aws_vpc" "main" {
        cidr_block = var.vpc_cidr_block

        tags = {
            Name = "${var.environment_stage} VPC"
        }
    }

    - resource "aws_subnet" "web_subnet" {
        cidr_block = var.subnet_cidr_block
        vpc_id = aws_vpc.main.id
        availability_zone = var.subnet_availability_zone_1

        tags = {
            Name = "${var.environment_stage} subnet for VPC"
        }
    }

19. this is how they are applied into your config files
    -var & -var-file
    *.auto.tfvars
    terraform.tfvars & production.tfvars
    environment variables "TF_VAR_*"

20. A route table is a virtual router within your vpc, it controls routing for all subnets within the vpc
    An Internet Gateway: it allows communication between instances in a Virtual Private Cloud (VPC) and
        the internet

21. - An aws_security_group is an aws managed service that determines if internet traffic are permitted
        into or out of an instance
    - aws_security_group = You have to create this one by yourself using the resource "aws_security_group"
      aws_default_security_group = This one comes by default when you create the VPC, it is always better to leave
        this one alone and create new security groups (i.e aws_security_group)

22. 
    - add a variable to your variables.tf and terraform.tfvars file: "public_cidr"
        in variables.tf
        variable "public_cidr" {
            description = "everyone is allowed to visit this EC2 instance"
            type = list(string)
            default = ["0.0.0.0/0"]
        }
        in terraform.tfvars
        public_cidr = ["0.0.0.0/0"]

    - use the variable to create an aws_security_group
        resource "aws_security_group" "ec2_security" {
            vpc_id = aws_vpc.main.id
        
            ingress = [
                # for ssh connections
                {
                    from_port = 22
                    to_port = 22
                    protocol = "tcp"
                    cidr_block = var.public_cidr
                    description = "for ssh connections to the ec2"
                    ipv6_cidr_blocks = []
                    self             = false
                    security_groups  = null
                    prefix_list_ids  = null
                },
                # for http connection on port 80
                {
                    from_port = 80
                    to_port = 80
                    protocol = "tcp"
                    cidr_block = var.public_cidr
                    description = "for http connections to the ec2 on port 80"
                    ipv6_cidr_blocks = []
                    self             = false
                    security_groups  = null
                    prefix_list_ids  = null
                }
                # for http connection on port 8080
                {
                    from_port = 80
                    to_port = 80
                    protocol = "tcp"
                    cidr_block = var.public_cidr
                    description = "for http connections to the ec2 on port 8080"
                    ipv6_cidr_blocks = []
                    self             = false
                    security_groups  = null
                    prefix_list_ids  = null
                }
            ]

            # for all outgoing request
            egress = [{
                from_port = 0
                to_port = 0
                protocol = "-1" # any type of request
                cidr_block = ["0.0.0.0/0"]
            }]
        
            tags = {
                Name = "EC2 security group"
            }
        }

    - Read: explanation of some of the options used in the ingress and egress
        1. ipv6_cidr_blocks = [] ?
          - What is ipv6_cidr_blocks:
            This field defines the list of allowed IPv6 CIDR blocks for the rule. IPv6 CIDR blocks specify which
            range of IPv6 addresses are allowed to access the resource.
          - Why is it empty ([]) ?
            The ipv6_cidr_blocks is empty because the rule is likely intended for IPv4 connections only. Since no
            IPv6 addresses are specified, the field is left empty (or set to an empty list []). If you wanted to
            allow IPv6 access, you would provide a valid IPv6 CIDR block here, such as "2001:0db8::/32".
        
        2. self = false
          - What is self ?
            The self field specifies whether the rule applies to traffic originating from the same security group.
            When self = true, the rule allows traffic between instances that are associated with the same security group.
            This is useful for cases where you want to allow internal communication between instances in the same security group.
            The rule is set to false, meaning it does not allow traffic from instances within the same security group

        3. security_groups = null
          - What is security_groups?
            The security_groups field is used to specify which other security groups can send traffic to the instances
            associated with this rule. This allows you to restrict access to specific groups, as opposed to opening up
            access to all IPs or CIDR blocks
          - Why is it set to null?
            Setting security_groups = null means that this rule is not specifically restricting traffic based on other
            security groups. In other words, it does not restrict access to instances from other security groups.
            If you had specific security groups to allow, you would list them here
        
        4. prefix_list_ids = null
          - What is prefix_list_ids?
            The prefix_list_ids field allows you to specify AWS-managed prefix lists (collections of IP address ranges)
            to use in the rule. AWS offers managed prefix lists that include ranges for services like
            Amazon S3, CloudFront, and others. This allows you to define rules based on these predefined IP address ranges
            rather than manually specifying CIDR blocks
          - Why is it set to null?
            Setting prefix_list_ids = null means that the rule is not using any AWS-managed prefix lists. If this field were
            set with the ID of a prefix list (e.g., "pl-12345678"), it would allow traffic from the IP addresses included
            in that prefix list.
        
        5. Summary of All Fields:
          - ipv6_cidr_blocks = []: No IPv6 addresses are specified, implying the rule is for IPv4 traffic.
          - self = false: Traffic from instances within the same security group is not allowed.
          - security_groups = null: No specific security groups are specified, meaning the rule does not restrict
            based on other security groups
          - prefix_list_ids = null: No AWS-managed prefix lists are being used, meaning no predefined IP address
            ranges are referenced
          - {Each of these fields can be customized to control traffic flow based on different security needs}

23.
    resource "aws_instance" "my_vm" {
        ami =                           "ami-4094940048585fse2"
        instance_type =                 "t3.micro"
        subnet_id =                     aws_subnet.web_subnet.id
        vpc_security_group_ids =        [aws_security_group.ec2_security.id]
        associate_public_ip_address =   true
        key_name =                      "production_ssh_key"
        user_data =                     file("./entry_script.sh")
        count = 2

        tags = {
            Name = "My EC2 instance - Selling town EC2_1"
        }
    }

24. ssh-keygen -t rsa -b 2048 -C 'test key' -N '' -f ~/.ssh/test_rsa {
        -t rsa: Specifies the type of key to create, in this case, RSA.
        -b 2048: Sets the number of bits in the key, here 2048 bits.
        -C 'test key': Adds a comment to the key.
        -N '': used for encryption. Sets an empty passphrase for the key (not recommended for all use cases due to security reasons).
        -f ~/.ssh/test_rsa: Specifies the filename of the key file. and where to save the key
    }

25. chmod 400 ~/.ssh/test_rsa

26. 
    resource "aws_key_pair" "test_ssh_key" {
        key_name = "testing_ssh_key"
        public_key = file(<path_to_file>) (i.e C:/Users/STANLEY/.ssh/test_rsa.pub)
    }

    resource "aws_instance" "my_vm" {
        ...
        key_name = aws_key_pair.test_ssh_key.key_name
        ...
    }

27. ssh -i ~/.ssh/test_rsa ec2-user@<PublicIP>
27.1 sudo su - (i.e sudo switch user - where dash(-)=root)

28. 
    in variables.tf we do:
    variable "ssh_pubic_key_path" {
        description = "The path to where the ssh key is stored on my local computer"
        type = string
    }

    in terraform.tfvars we do:
    ssh_pubic_key_path = "C:/Users/STANLEY/.ssh/test_rsa.pub"

    in main.tf we do:
    resource "aws_key_pair" "test_ssh_key" {
        key_name = "testing_ssh_key"
        public_key = file(var.ssh_pubic_key_path)
    }

    Note: if you change the public_key value, a new instance is created, each instance belong to the key
        it was created with

29. Data sources are like a function, and their goal is to fetch data from cloud providers. An example is a
    list of ami images from an availability_zone
    -
    data "aws_ami" "latest_amazon_linux2" {
        owners = ["amazon"]
        most_recent = true   # if more than one image is returned, terraform will use the most recent image

        filter {
            name = "name"    # filter the result using the items in the value[] list below
            values = ["amzn2-ami-kernel-*-x86_64-gp2"]
        }

        filter {
            name = "architecture"
            values = ["x86_64"]
        }
    }
    {Now we need to update our aws_instance with the dynamic ami id}
    resource "aws_instance" "my_vm" {
        ami = data.aws_ami.latest_amazon_linux2.id
        ...
    }

30. outputs.tf (you can also do output.tf)

31. in your outputs.tf
    output "ec2_public_ip_address" {
        description = "The public IP address of the EC2 instance."
        value       = aws_instance.my_vm.public_ip
    }

    output "ec2_ami" {
        description = "The ami of our ec2 instance"
        value = aws_instance.my_vm.ami
    }

    output "vpc_id" {
        description = "ID of the main VPC"
        value = aws_vpc.main.id
    }

    output "ec2_instance_id" {
        description = "The ID of the EC2 instance."
        value       = aws_instance.my_vm.id
    }

32.
    - terraform output
    - terraform output ec2_public_ip_address
    - terraform output -json
    - terraform output -json > output.json
    - if you want to hide an output from display in the terminal, set sensitive = true, i.e {
        output "ec2_public_ip_address" {
            description = "The public IP address of the EC2 instance for ebay"
            value = aws_instance.my_vm.public_ip
            sensitive = true
        }
    }

33. Terraform state is a critical component of Terraform's infrastructure management process. It is a file that
    tracks the current state of your infrastructure as managed by Terraform. Terraform state acts as d source of
    truth for your managed infrastructure, enabling Terraform to track, update, and manage resources effectively.
    For more {
        1. **Purpose**: The state file maintains a mapping between your Terraform configuration and the actual
            resources deployed in your infrastructure. It helps Terraform understand what resources exist, their
            configurations, and how they relate to each other.

        2. **Storage**: By default, the state file is named `terraform.tfstate` and is stored locally in your
            working directory. However, in team environments or production settings, itâ€™s common to use remote
            state storage solutions like Amazon S3, Azure Blob Storage, or Terraform Cloud for better management
            and collaboration.

        3. **Usage**: During operations like `terraform apply`, Terraform reads the state file to determine the
            changes that need to be made to your infrastructure. It then updates the state file with the latest
            information after applying the changes.

        4. **Locking and Security**: For remote state, locking mechanisms prevent simultaneous updates.
    }
    ---
    ---
    terraform state is stored in a terraform.tfstate file

34. - terraform show
    - terraform show | grep -A 20 aws_vpc -: means, show all resources, then pipe the result to the grep
        command and then search for aws_vpc, if you find the result, then show 20lines after it(i.e A20)
    - terraform show -target=<resource_address>
        e.g terraform show -target=aws_instance.my_vm
    - terraform show <plan_file> e.g terraform show tfplan

35. terraform state
    - terraform state list
    - terraform state show <resource_address>
        e.g terraform state show aws_instance.my_vm
    - terraform state pull
    - terraform state list | grep 'module.<module_name>'

36. running commands on you ec2 instance
    - which is the best for running commands on your ec2 instance (cloud-init or ansible)
        - For initial boot time configuration, i.e if you need to perform simple one time setup tasks when a VM is
            first launched, then use "cloud-init". it is the best tool. it is lightweight, easy to use and designed
            for exactly this Purpose
        - For ongoing configuration management, if you need to manage the configuration of servers throughout their
            lifecycle, including making changes, applying updates, or orchestrating complex tasks, Then use Ansible,
            it provides more power, flexibility and scalability
        # using both
            Many organization use both tools in tandem:
            ~ cloud-init for initial bootstrapping and configuration e.g installing docker, apache, httpd & ansible.
            ~ ansible for ongoing configuration management and complex orchestration after the initial setup is completed

    - create an entry-script.sh configuration that can be used in your ec2 instance

    - not_needed: Read: read about some of the highlights of using cloud-init
        ~ it is the standard for customizing cloud instances on spin up
        ~ it runs on most linux distributions and cloud providers
        ~ cloud-init can run per-instance or per-boot configurations

    - not_needed: create a cloud-init configuration in yaml called "web-app-template.yaml" and use it in your ec2
        - ./task/web-app-template-v1.yml - from teacher
        - ./task/web-app-template-v2.yml - from codeium

    - not_needed: explain the code in your cloud-init configuration
        -
        1. Adding groups to the system
        groups:
            - devops: [root, sys]
            - hashicorp
        The above is adding 2 groups to the system, it added the "devops" group with 2 users "root" & "sys",
        and also created another group "hashicorp" but without any users in this group

        -
        2. Adding users to the system. Users are added after groups are added
        users:
            - default               # this will be the default system user
            - name: terraform       # the name of the user
                gecos: terraform    # the GECOS field(General Electric Comprehensive Operating Supervisor)
                                        typically contains the full-name or the description of the user, but
                                        here, we used the same name as the username "terraform"
                shell: /bin/bash    # the default shell of the user
                primary_group: hashicorp        # adds a default group for the user
                sudo: ALL=(ALL) NOPASSWD:ALL    # this allows the user to execute any command as any user
                                                    (including as root user) without any password
                groups: users, admin    # this adds the user to other groups (users & admin group)
                lock_passwd: false      # this means the user account is not locked, so the user will not be
                                            required a password before they can login
                ssh_authorized_keys:
                    - ssh-rsa <key>
                    # the ssh_authorized_keys section allows you to Specify ssh-keys for the user, enabling
                        secure ssh access without using a password
                    # for the ssh-rsa <key>, replace the <key> with your rsa public key
        
        -
        3. Downloading and installing packages
        packages:
            - httpd
            - docker
        The above will download and install 2 packages, Apache httpd and Docker

        -
        4. Running commands on the system
        runcmd:
            - sudo systemctl start httpd
            - sudo systemctl start docker
            - sudo usermod -a -G docker ec2-user
            - sudo docker run -p 8080:80 nginx
        sudo: using sudo grants you all privileges so you can run & any commands you want
        systemctl: is a command used to control the systemd and system service manager
        "sudo usermod -a -G docker ec2-user":
            usermod: is used to modify the privileges and attributes of a user
            -a : user to append a user to a group without removing the user from another group
            -G : compliments -a, Specifies the group that the user should be added to
            {the "ec2-user" was added to the "docker" group, which allows the ec2-user to run any docker commands
             without needing to use sudo}
        "sudo docker run -p 8080:80 nginx":
            spins up a new docker container using an "nginx" image, the container will listen to request on port 8080,
            and map the request to nginx port 80

    - not_needed: use the cloud-init in your ec2
        data "template_file" "user_data" {
            # path to where the configuration is located relative to your main.tf
            template = file("./task/web-app-template-v1.yml")
        }

        resource "aws_instance" "my_vm" {
            ...
            # user_data = file("entry-script.sh") he commented this line out
            user_data = data.template_file.user_data.rendered
            ...
        }


37.
    - explain why this error occurred and how can it be solved
        so the main issue is that this is a new_project and all the terraform code here was copied from another
        terraform project_1, and in "project_1", the "aws_key_pair" has already been used
        (for every new ssh aws_key_pair you use, amazon creates it on your aws account using the key_name you used
        in creating the aws_key_pair), so now this new_project is trying to create a new aws_key_pair on aws using
        the same key_pair_name & the key already does exist, that's why an error occurred
        -
    - how you solve the above problem
        To solve this problem {the teacher says it is usually a common problem}, you have to import the aws_key_pair
        into this new_project, here is what you'll do:

        terraform import aws_key_pair.<local_name> <key_pair_name>
        -
        <local_name>: is the name you want to give to the resource in your Terraform configuration
        <key_pair_name>: is the actual name of the key pair in AWS.

        e.g:
        terraform import aws_key_pair.my_key_pair testing_ssh_key
        -
        resource "aws_key_pair" "my_key_pair" {
            key_name   = "testing_ssh_key"
            public_key = file("<path_to_file>")
        }
    - Read: read how you can find your key pairs on aws
        - Go to your aws console, go to the vpc section
        - Go to your Network & Security tab
        - Click on Key Pairs {it's a Link - here you'll see all d key pairs you've used in all ur different projects}

38.
    - login to your ec2 instance
        ssh -i ~/.ssh/test_rsa ec2-user@<PublicIP>

    - verify if all the users were created successfully
        tail /etc/passwd
        -
        you'll see stuffs like:
        terraform:x:1000:1000:terraform:/home/terraform:/bin/bash           # the user we created with cloud-init
        ec2-user:x:1001:1003:EC2 Default user :/home/ec2-user:/bin/bash     # default user created by aws_instance
        apache:x:48:48 ...                                                  # user created by apache

    - verify if all the groups were created successfully
        tail /etc/group
        -
        hashicorp:x:1000                # 1
        devops:x:1001:root              # 2
        admin:x:1002:terraform          # 3
        ec2-user:x:1003                 # 4
        apache:x:48                     # 5
        docker:x:992:ec2-user           # 6
        -
        1. Group name: hashicorp, Password: x, Group ID (GID): 1000, Members: No members
        2. Group name: root, Password: x, Group ID (GID): 1001, Members: "root user"
        3. Group name: admin, Password: x, Group ID (GID): 1002, Members: "terraform user"
        4. Group name: ec2-user, Password: x, Group ID (GID): 1003, Members: No members
        5. Group name: apache, Password: x, Group ID (GID): 48, Members: No members
        6. Group name: docker, Password: x, Group ID (GID): 992, Members: "ec2-user user"

    - check if your httpd is running
        systemctl status httpd

    - check if your docker is running
        systemctl status docker

    - use your browser and confirm that the ec2 instance is accessible from the web
        http://35.159.20.29/
    - use your browser and confirm that the docker on your ec2 instance is running
        http://35.159.20.29:8080/

39. Read: read about how to troubleshot & logging in terraform
    ~ Always run "terraform fmt" and "terraform validate" before you run terraform "plan" or "apply".
    ~ To log your terraform operations in your production environment, you have to set your "TF_LOG"
        to either TRACE, DEBUG, INFO, WARN or ERROR 
    ~ Then to save all your logs to a file, you have to set TF_LOG_PATH to filename:
        e.g
            in your terminal do:
            export TF_LOG=DEBUG
            export TF_LOG_PATH=terraform.log
    ~ there are also other options such as TF_LOG_CORE & TF_LOG_PROVIDER, you can research them at your
        own convenience

================
SECTION 2 BEGINS
================

1. List the different value types in terraform
    - List simple types
        number, string, bool, null
    - List collection types
        list, map, set
    - List structural types
        tuple, object

2. Collection types
    - Give an example of using "list"
        variable "av_zones" {
            description = "availability_zones in the region"
            type = list(string)
            default = ["eu-central-1a", "eu-central-1b", "eu-central-1c"]
        }
        ---
        in main.tf
        resource "aws_subnet" "web_subnet" {
            availability_zone = var.av_zones[1] # eu-central-1b
        }

    - Give an example of using "maps"
        variable "amis" {
            description = "..."
            type = map(string)
            default = {
                "eu-central-1a" = "ami-something-eu-central"
                "us-west-1" = "ami-something-us-west-1"
            }
        }
        ---
        in main.tf
        resource "aws_instance" "my_vm" {
            ami = var.amis[var.region]
            or
            ami = var.amis["${var.region}"]
        }
    
3. Structural types
    - Give an example of using "tuple"
        variable "my_vm" {
            description = "..."
            type = tuple([string, number, bool])
            default = ["t2.micro", 1, true]
        }
        ---
        in main.tf
        resource "aws_subnet" "my_vm" {
            ami = var.amis[var.region]
            instance_type = var.my_vm[0]
            count = var.my_vm[1]
            associate_public_ip_address = var.my_vm[2]
        }

    - Give an example of using "object"
        variable "egress_dsg" {
            type = object({
                from_port = number
                to_port = number
                protocol = string
                cidr_block = list(string)
            })
            default = {
                from_port = 0,
                to_port = 65365, # maximum amount of ports
                protocol = "tcp",
                cidr_block = ["100.0.0.0/16", "200.0.0.0/16"]
            }
        }
        ---
        in main.tf
        resource "aws_security_group" "ec2_security" {
            ...
            egress {
                from_port = var.egress_dsg["from_port"]
                to_port = var.egress_dsg["to_port"]
                protocol = var.egress_dsg["protocol"]
                cidr_block = var.egress_dsg["cidr_block"]
            }
        }

4. The count meta-argument
    - What is count in terraform and what can it be used for
        - count is used to manage similar resources, you can use it to automatically scale up/down resources
        - count and for_each are looping techniques

    - Use count to create 3 aws instances
        resource "aws_instance" "my_vm" {
            ami = ....
            instance_type = "t2.micro"
            count = 3
        }

    - Using count & variable: "users" (list(string), a list of usernames), create 4 IAM users
        variable "users" {
            type = list(string)
            default = ["demo-user", "stanley", "edward", "chukwu"]
        }

        resource "aws_iam_user" "level2user" {
            name = "${element(var.users, count.index)}"
            path = "/system/"
            count = "${length(var.users)}"
        }
        {You can go to the terraform doc to see how you can customize the IAM users and set level/roles and what
          the users can have access to}

    - explain the element and length functions
        ~ element is used to retrieve an item from a list
        ~ length returns the total number of items in a list

    - when you do count=0 in any resource, what will happen when you run terraform apply
        when you set count = 0 in a resource block, that resource will not be created, if the resource already
        exists, the resource will be destroyed

5.  - Read: the for_each meta-argument {No need to master}
        for_each was introduced to overcome the downside of count, it works with either sets or maps
        e.g:
        variable "users" {
            type = list(string)
            default = ["demo-user", "stanley", "edward", "chukwu"]
        }
        -
        resource "aws_iam_user" "test" {
            for_each = toset(var.users) # toset converts the list(string) into a set
            name = each.key # each.key iterates over the set and grabs the key
        }

6.  
    - Read: take a look at looping with dynamic block
        dynamic block is another way of looping that can save us time
        e.g:
        variable "ingress_ports" {
            description = "List of Ingress Ports"
            type = list(number)
            default = [22, 80, 25, 443, 993, 8080]
        }
        -
        in main.tf
        resource "aws_security_group" "ec2_security" {
            vpc_id = aws_vpc.main.id

            dynamic "ingress" {
                for_each = var.ingress_ports
                content {
                    from_port = ingress.value
                    to_port = ingress.value
                    protocol = "tcp"
                    cidr_blocks = ["0.0.0.0/0"]
                }
            }

            egress {
                from_port = 0
                to_port = 0
                protocol = "-1"
                cidr_blocks = ["0.0.0.0/0"]
            }
        }

    - Read: what happens when you run terraform apply
        if you run terraform apply, your security group will be created with all the ports provided, you can
        go to your aws console to confirm this:
        ~ go to aws > search for "VPC" > SECURITY > SECURITY_GROUPS
        ~ select the security group you created, click on inbound rules
            here you'll see all the rules you've created with the dynamic block

    - Read: changing the iterator name
        you can set an iterator name to use in accessing the for_each values
        dynamic "ingress" {
            for_each = var.ingress_ports
            iterator = iport
            content {
                from_port = iport.value
                to_port = iport.value
                protocol = "tcp"
                cidr_blocks = ["0.0.0.0/0"]
            }
        }

7. Conditional Expressions
 - let's say we want to spin up different instances depending on our current environment, use
    terraform conditions and variables to check if we should launch an ec2_instance for testing
    or for production
    -
    variable "is_test" {
        description = "..."
        type = bool
    }
    -
    in terraform.tfvars
    is_test = true
    -
    in main.tf
    resource "aws_instance" "test_server" {
        ami = "..."
        instance_type = "t3.micro"
        count = var.is_test == true ? 1 : 0
    }
    resource "aws_instance" "production_server" {
        ami = "..."
        instance_type = "t3.large"
        count = var.is_test == false ? 1 : 0
    }

    You can also do stuffs like:
    instance_type = var.env_stage == "dev" ? "t3.micro" : "t3.large"

8. Locals
    - in what files can you save your locals
        You can define your locals in a file called locals.tf, you can also store them in variables.tf and
        lastly in main.tf(i.e if you do not have too many locals)

    - give an example of using locals
        in locals.tf
        locals {
            owner = "DevOps Corp Team"
            project = "online store"
            cidr_blocks = ["172.16.10.0/24", "172.16.20.0/24", "172.16.30.0/24"]
            common-tags = {
                Name = "Dev"
                Environment = "development"
                version = 1.10
            }
        }
        -
        in main.tf
        resource "aws_vpc" "dev_vpc" {
            cidr_block = "10.0.0.0/16"
            tags = local.common-tags
        }
        resource "aws_subnet" "dev_subnet" {
            vpc_id = aws_vpc.dev_vpc.id
            cidr_block = local.cidr_block[0]
            availability_zone = "eu-central-1a"
        }
        resource "aws_internet_gateway" "dev_igw" {
            vpc_id = aws_vpc.dev_vpc.id
            tags = {
                Name = local.common-tags["Name"]
                version = local.common-tags["Version"]
            }
        }

9. Terraform built-in functions
    If you go to the terraform documentation, navigate to the functions section, you'll see all of the functions provided
    by terraform. You cannot create your own custom functions in terraform

    - lookup function
        it retrieves a "value" from a "map" using a "key" from the map
        lookup(<map>, <key_name>)
        e.g ami = lookup(var.ami, var.region)

    - difference btw lookup & element function
        lookup retrieves an item from a map using the key of the value
        element retrieves an item from a list using the index of the value

    - file function
        The file function reads the content of a file and returns it as a string
        e.g user_data = file("entry-script.sh")

    - formatDate & timestamp()
        timestamp() is used to retrieve the current time
        formatDate() is used in to format the date received from timestamp()
        e.g:
        in locals.tf
          locals {
            time = formatDate("DD MM YYYY hh:mm", timestamp())
          }
        in outputs.tf
          output "CurrentTime" {
            description = "Current Date and Time"
            value = local.time
          }

10. Give an example of using splat expressions
    Lets say for example we have
    resource "aws_instance" "my_vm" {
        ami = "ami..."
        instance_type = "t3.micro"
        count = 3
        associate_public_ip_address = true
    }

    The above will create 3 ec2 instances, so how do we dynamically display the info of these aws_instance's. Say we wanted
    to display the ip addresses, we might do:
    output "ip_address_1" {
        value = aws_instance.my_vm[0].public_ip
    } 

    imagine if we created upTo 10vm, this would take forever, so we can use splat-expressions to dynamically loop through
    the vm's details, i.e:
    output "ip_addresses" {
        value = aws_instance.my_vm[*].public_ip
    }

11. Read: Terraform remote state
    - working with amazon s3 to store your state file
      you can use terraform to create your s3 bucket or you can go to the terraform website and create your bucket there
      after creating the s3_bucket and dynamodb for state locking, add the below to your terraform "main.tf" or another file,
      maybe "backend.tf":

      terraform {
        required_providers { ... }

        backend "s3" {
          bucket = <name-of-bucket>
          key = <name-of-file> e.g s3_backend.tfstate
          region = "eu-north-1"
          encrypt = true
          dynamodb_table = <name-of-table>
        }
      }

    - using dynamodb to implement state locking
      You can go to aws and create a dynamodb table from there, or you can use terraform to create a dynamodb_table
      The dynamodb_table will help with state locking to prevent simultaneous updating of a state file

    - Read: working with terraform cloud to store your state file
        You can also you hashicorp cloud for storing of your state file, if you'd like to use hashicorp cloud, then see
        Terraform 2 - Book35.1 - page-10

12. 